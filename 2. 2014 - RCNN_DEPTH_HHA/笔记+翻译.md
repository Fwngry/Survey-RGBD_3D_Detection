# 2014 - RCNN_Depth

> 2.5D方法：RCNN-Depth
>
> 亮点：HHA编码、CNN提取特征+SVM分类器、RCNN的RGBD版本
>
> 数据集：NYUD2
>
> 任务：目标检测、实例分割、语义分割
>
> 输出：2D框
>
> Open source:https://github.com/s-gupta/rcnn-depth

![image-20210522094233221](https://oj84-1259326782.cos.ap-chengdu.myqcloud.com/uPic/2021/05_22_05_22_05_22_image-20210522094233221.png)

在R-CNN框架中使用2.5D区域的proposal上计算出来的特征

深度图应该如何编码以用于CNN？CNN是直接在原始深度图上工作，还是对输入进行转换，以便CNN更有效地学习？

检测性能：在RGB-D物体检测中获得了56%的相对改进

获取proposal：Proposals Sort - MCG

实验设置：对比DMP作为Baseline - disparity - CNN+[RGB/HHA/RGB&HHA+pool5/fc6/fc7)

<img src="https://oj84-1259326782.cos.ap-chengdu.myqcloud.com/uPic/2021/05_22_image-20210522100549875.png" alt="image-20210522100549875" style="zoom:67%;" />

## 摘要

在本文中，我们研究了「使用语义丰富的图像和深度特征」对RGB-D图像进行物体检测的问题。<u>我们提出了一种新的深度图像的地心嵌入，除了水平差异外，还对每个像素的地面以上高度和重力角进行编码。我们证明这种地心嵌入比使用原始深度图像来学习卷积神经网络的特征表示效果更好。</u>

然后，我们将重点放在实例分割的任务上，为属于我们检测器发现的物体实例的像素贴上标签。对于这项任务，我们提出了一种决策森林方法，使用查询形状和地心位置特征的一系列单项和双项测试，将检测窗口中的像素分类为前景或背景。

最后，我们在现有的superpixel classiﬁcation framework中使用我们的物体检测器的输出来进行语义场景分割，并在我们研究的物体类别中取得了比目前最先进的24%的相对改进。我们相信，像本文中提到的这些进展将促进感知技术在机器人等领域的应用。

## 1.引言

RGB-D图像使人们能够计算深度和法线梯度[18]，我们将其与[9]中的结构化学习方法相结合，产生明显改善的轮廓。然后，我们使用这些RGB-D轮廓，通过计算深度和彩色图像的特征来获得2.5D区域候选者，用于Arbel´aez等人[1]的多尺度组合分组（MCG）框架。这个模块是最先进的RGB-D建议生成模块。

（RGB-D图像计算轮廓结构、计算深度和彩色图像的特征，获得2.5D区域候选者）

在RGB图像上预训练的大型CNN可以适应于为深度图像生成丰富的特征。我们建议用三个通道（水平差距、离地高度和与重力的夹角）来表示深度图像，并表明这种表示方法允许CNN学习比单独使用差距（或深度）更强的特征。

> 法线梯度。我们在两个尺度上计算法线梯度（对应于在半径为3和5像素的半圆盘中确定一个局部平面），并将其作为额外的梯度图。
>
> 地心姿态。我们计算每个像素的离地高度和与重力的角度（使用我们在[18]中提出的算法。这些特征使决策树能够利用额外的规律性，例如，地面上的亮度边缘不像其他地方的亮度边缘那么重要。
>
> 更丰富的外观。我们观察到，NYUD2数据集的外观变化有限（因为它只包含室内场景的图像）。为了使模型具有更好的通用性，我们在RGB图像上添加了通过运行[9]的RGB边缘检测器（该检测器是在BSD上训练的）产生的软边缘图。

本文重点解决实例分割

## 2.5D区域建议

### 2.1 轮廓检测

RGB-D轮廓检测是一项经过充分研究的任务[9,18,29,33]。在这里，我们结合了两个领先的方法的想法，[9]和我们过去在[18]的工作。

[9]产生了更好的局部轮廓，捕捉到了细微的细节，但往往会错过[18]很容易发现的正常的不连续性（例如，考虑图2左边部分的墙壁和天花板之间的轮廓）。我们提出了两种方法的综合，将[18]的特征与[9]的学习框架相结合。具体来说，我们增加了以下特征。

### 2.2 Proposals Sort - MCG

从改进的轮廓信号中，我们通过将MCG推广到RGB-D图像来获得物体建议。用于RGB图像的MCG[1] Multiscale Combinatorial Grouping ，使用基于彩色图像和区域形状的简单特征来训练<u>随机森林回归器</u>，对物体建议进行<u>排序</u>。我们遵循相同的范式，但提出了在每个提议中对深度图像进行计算的<u>额外几何特征</u>。

额外几何特征：

(1) 区域内各点的差距、离地高度、重力角和世界（X、Y、Z）坐标的平均值和标准偏差；(2) 区域的（X、Y、Z）范围；(3) 区域离地的最低和最高高度；(4) 垂直表面、朝上的表面和朝下的表面上的像素比例；(5) 沿房间顶视图的某个方向的最低和最高标准偏差。

除了在[1]中已经计算出的来自二维区域形状和颜色图像的14个特征外，我们为每个区域获得了29个几何特征。请注意，一个区域的这些特征的计算是通过超像素来分解的，可以通过首先计算超像素上的一阶和二阶矩，然后适当地将它们结合起来，从而巧妙地完成。

## 3. RGBD检测器

### 3.1 为特征学习的深度图像编码

给定一个深度图像，它应该如何编码以用于CNN？CNN应该直接在原始深度图上工作，还是对输入进行转换，以便CNN更有效地学习？

我们建议用每个像素的三个通道对深度图像进行编码：水平差异、离地高度以及像素的局部表面法线与推断的重力方向的角度。我们把这种编码称为HHA。后面两个通道是用[18]中提出的算法计算的，所有的通道都是线性缩放的，以将整个训练数据集的观察值映射到0到255的范围。

HHA表示编码地心位置的属性，强调图像中互补的不连续性（深度、表面法线和高度）。此外，CNN不太可能自动学会直接从深度图像中计算这些属性，特别是当训练数据非常有限时，如NYUD2数据集的情况。

### 3.2 合成数据进行扩充：训练集过小

一个重要的观察结果是，我们在NYUD2数据集中拥有的监督训练数据量比PASCAL VOC数据集小一个数量级（400张图像，而PASCAL VOC 2007为2500张）。为了解决这个问题，我们产生更多的数据用于训练和ﬁnetuning网络。有多种方法可以做到这一点：

1. 对已有的场景进行网格化处理，并从新的视点渲染场景（噪声过大）

2. 使用光流的注释来使用数据集中的附近视频帧的数据（序列相似）

3. 使用互联网上的全三维合成CAD物体模型，并将其渲染成场景。

因此，我们采用了第三种方法，渲染了NYUD2的三维注释，可从[17]中获得，以生成不同视角的合成场景。在生成这些数据时，我们还模拟了Kinect的量化模型（渲染的深度图像被转换为量化的差异图像，低分辨率的白噪声被添加到差异值中）。

### 3.3 实验

我们使用NYUD2数据集，并使用第2.3节中描述的标准数据集分割成训练、评价和测试。该数据集带有语义分割注释，我们将其包围在一个紧密的盒子中以获得边界盒注释。我们使用数据集中的主要家具类别，如椅子、床、沙发、桌子（列于表2）。

实验设置：训练我们的模型有两个方面：微调卷积神经网络的特征学习，以及训练线性SVM的对象建议分类。

微调：我们使用Caﬀe CNN库[21]遵循[16]中的R-CNN程序。我们从一个在更大的ILSVRC 2012数据集上预训练过的CNN开始。

对于ﬁnetuning：<u>学习率</u>初始化为0.001，每20k次迭代减少10倍。我们进行了3万次迭代，这在NVIDIA Titan GPU上需要大约7个小时。按照[16]，如果重叠度大于0.5，我们给每个训练实例贴上具有最大重叠度的类，否则就贴上背景。所有的ﬁnetuning都是在训练集上完成的。

SVM训练：为了训练线性SVM，我们从池化层5（pool5）、全连接层6（fc6）或全连接层7（fc7）计算特征。在SVM训练中，我们将正面例子定义为来自目标类的gt框，负面例子定义为与该类gt实例的交集小于0.3的框。使用liblinear[12]在训练集上进行了SVM超参数C=0.001，B=10，w1=2.0的训练。我们报告了控制实验中Val集的性能（检测平均精度APb）。

在最后的实验中，我们在trainval上进行训练，并在测试集上报告与其他方法的性能比较。在测试时，我们从网络中的fc6层计算出特征，应用线性分类器，并对输出进行非最大抑制，以获得测试图像上的一组稀疏检测结果。

我们使用PASCAL VOC箱体检测的平均精度（按照[19]中介绍的一般化方法表示为APb）作为性能指标。

结果列于表2。作为基线，我们报告了<u>最先进的基于非神经网络的检测系统</u>，<u>可变形部件模型（DPM）</u>[14]的性能。首先，我们在RGB图像上对DPM进行了训练，得到的平均值为8.4%