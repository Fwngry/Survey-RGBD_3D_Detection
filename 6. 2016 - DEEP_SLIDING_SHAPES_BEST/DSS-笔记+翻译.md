> Open Source https://github.com/shurans/DeepSlidingShape
>
> 3D表示+3D方法：3D Faster RCNN
>
> 纯3D输入输出，NYUv2数据集和SUN RGB-D数据集
>
> 3D表示：TSDF
>
> 多尺度：3D Region Proposal Network三维区域建议网络（RPN）
>
> 色彩+几何：joint Object Recognition Network联合物体识别网络（PRN）

## Abstract

我们介绍了Deep Sliding Shapes，这是一个3D ConvNet方案，将RGB-D图像中的3D体积场景作为输入，并输出3D物体边界盒。

在我们的方法中，我们提出了第一个三维区域建议网络（RPN），从几何形状中学习物体性，以及第一个联合物体识别网络（ORN），以提取三维的几何特征和二维的颜色特征。特别是，我们通过在两个不同的尺度上训练一个正交的RPN和一个ORN来处理各种尺寸的物体，以回归三维边界盒。

实验表明，我们的算法在mAP方面比最先进的算法高出13.8倍，比原来的滑动图形算法快200倍。

<img src="https://oj84-1259326782.cos.ap-chengdu.myqcloud.com/uPic/2021/05_25_image-20210525150000081.png" alt="image-20210525150000081" style="zoom:50%;" />

## introduction

二维边界框对于某些任务是有用的，比如物体检索，但对于在真实的三维世界中做任何进一步的推理来说，它是相当不满意的。

在本文中，我们专注于RGB-D图像中的正负三维物体检测任务，其目的是产生一个物体的三维边界框，在物体的全部范围内给出真实世界的尺寸，而不考虑截断或遮挡。例如，这种识别在机器人应用的感知-操纵循环中更为有用。但是，为预测增加一个新的维度，大大地扩大了搜索空间，并使任务更具挑战性。

（3D表示+手工特征）天真地将二维检测结果转换为三维，效果并不理想（见表3和[10]）。为了很好地利用深度信息，有人提出了滑动形状[25]，在三维空间中滑动一个三维检测窗口。虽然它受到使用手工制作的特征的限制，但这种方法自然地将任务制定在三维空间中。

（2.5D方法：2D表示+2D RCNN）另外，Depth RCNN[10]采用了一种二维方法：通过将深度视为彩色图像的额外通道来检测二维图像平面中的物体，然后通过使用ICP对齐，将三维模型与二维检测窗口内的点相匹配。

考虑到现有的二维和三维方法，我们很自然地会问：哪种表示法更适合于三维正交物体检测，二维还是三维？目前，以2D为中心的Depth RCNN优于以3D为中心的Sliding Shapes。但是，也许Depth RCNN的优势来自于使用一个精心设计的深度网络，用ImageNet进行预训练，而不是其2D表示法。是否有可能通过同时利用3D的深度学习来获得一个优雅但更强大的3D表述呢？

在本文中，我们介绍了Deep Sliding Shapes，这是一个完整的三维表述，使用三维卷积神经网络（ConvNets）来学习物体建议和分类器。

【多尺度】我们提出了第一个<u>*3D Region Proposal Network三维区域建议网络（RPN）*</u>，它以三维体积场景为输入，输出三维物体建议（图1）。它被设计用来为具有不同尺寸的物体在两个不同的尺度上生成整个物体的模态建议。

【色彩+几何】我们还提出了第一个 <u>*joint Object Recognition Network联合物体识别网络（PRN）*</u>，使用二维ConvNet从颜色中提取图像特征，并使用三维ConvNet从深度中提取几何特征（图2）。这个网络也是第一个直接从三维建议中回归物体的三维边界盒的网络。

大量的实验表明，我们的三维ConvNets可以学习到比二维表示（如深度-RCNN中的HHA）更强大的几何形状编码表示（表3）。我们的算法也比Depth-RCNN和原来的Sliding Shapes快得多，因为它在测试时只需要在GPU中进行一次ConvNets的前向传递。

我们的设计充分利用了3D的优势。因此，我们的算法自然得益于以下五个方面：

1. 首先，我们可以预测三维边界框，而不需要从额外的<u>CAD</u>数据中确定模型的额外步骤。这优雅地简化了管道，加快了速度，并提高了性能，因为网络可以直接优化最终目标。
2. 其次，由于<u>遮挡、有限的视野</u>和投影造成的巨大尺寸变化，正交建议的生成和识别在二维中是非常困难的。但在三维中，由于来自同一类别的物体通常具有相似的物理尺寸，而且来自遮挡物的干扰会落在窗口之外，因此我们的三维滑动窗口建议生成可以自然地支持正交检测。
3. 第三，通过用3D表示形状，我们的ConvNet可以有机会在一个更好的<u>对齐空间</u>中学习有意义的3D形状特征。
4. 第四，在RPN中，接受场是以现实世界的<u>维度来自然表示</u>的，这对我们的架构设计有指导作用。
5. 最后，我们可以通过使用<u>曼哈顿世界的假设</u>来确定边界盒的方向，从而利用简单的三维背景先验。

虽然机会是令人鼓舞的，但三维物体检测也有几个独特的挑战：

1. 首先，三维体积表示法需要**更多的内存和计算**。为了解决这个问题，我们建议将三维区域建议网络与低分辨率的整个场景作为输入分开，而物体识别网络与每个物体的高分辨率输入分开
2. 第二，三维物理物体的边界框比基于像素的二维边界框的**尺寸变化更大**（由于摄影和数据集的偏差）[16]。为了解决这个问题，我们提出了一个<u>多尺度</u>的区域建议网络，用不同的接受场来预测不同尺寸的建议。
3. 第三，虽然来自深度的几何形状非常有用，但它们的信号通常**比彩色图像中的纹理信号频率低**。为了解决这个问题，我们提出了一个简单但有原则的方法，以联合纳入通过投射三维区域建议而得到的二维图像补丁的颜色信息。

### 1.1. 相关工作

深度ConvNets已经彻底改变了基于2D图像的物体检测。RCNN[8]、Fast RCNN[7]和Faster RCNN[18]是最成功的三个迭代版本。除了预测物体的可见部分，[14]进一步扩展了RCNN，以估计整个物体的正交箱。但他们的结果是二维的，而且只估计了物体的高度，而我们希望得到三维的正交箱。受二维的成功启发，本文提出了一个综合的三维检测管道，利用三维ConvNets对RGB-D图像的三维几何线索进行检测。

RGB-D图像中的2D物体检测：RGB-D图像的2D物体检测方法将深度作为附加到彩色图像的额外通道，使用手工制作的特征[9]、稀疏编码[2，3]或递归神经网络[23]。

深度RCNN[11, 10]：在RGB-D图像上使用深度ConvNets的物体检测器。

HHA+RCNN 他们通过将深度图编码为附加在彩色图像上的三个额外通道（具有地心编码：差距、高度和角度），将RCNN框架[8]扩展为基于颜色的物体检测。[10]通过将三维CAD模型与识别结果对齐，扩展了深度-RCNN，以产生三维边界盒。

3D表示+手工制作特征：三维物体检测器滑动形状[25]是一个三维物体检测器，在三维中运行滑动窗口，直接对每个三维窗口进行分类。然而，该算法使用了手工制作的特征，而且该算法使用了许多示范分类器，所以它的速度非常慢。最近，[32]也提出了RGB-D图像上的定向梯度云的特征。在本文中，我们希望用3D ConvNets来改进这些手工制作的特征表示，它可以从数据中学习强大的3D和颜色特征。

三维特征学习HMP3D[15]引入了一种层次化的稀疏编码技术，用于从RGB-D图像和三维点云数据中无监督地学习特征。该特征是在一个合成的CAD数据集上训练的，并在RGB-D视频中的场景标签任务上进行测试。<u>相比之下，我们希望用有监督的方式来学习三维特征，使用深度学习技术，这些技术被证明对基于图像的特征学习更有效。</u>

三维深度学习3D ShapeNets[29]引入了三维深度学习来为三维形状建模，并证明可以从大量的三维数据中学习到强大的三维特征。最近的几项工作[17, 5, 31, 13]也提取深度学习特征用于CAD模型的检索和分类。<u>虽然这些工作很有启发性，但它们都没有专注于RGB-D图像中的3D物体检测。</u>

区域提议:对于二维物体提议，以前的方法[27, 1, 11]大多是基于合并分割结果的。最近，Faster RCNN[18]引入了一种更有效的基于ConvNet的表述，这激发了我们使用ConvNets来学习3D物体性。对于三维物体的建议，[4]介绍了一个带有手工制作的特征的MRF公式，用于街道场景中的一些物体类别。<u>我们希望使用ConvNets从数据中学习一般场景的3D对象性。</u>

## 2 编码3D表示：TSDF

希望有一种方法可以在三维中自然编码几何形状，保留空间定位。此外，与使用手工制作的3D特征的方法相比，尽可能原始地编码3D几何图形的表示方法，并让ConvNets从原始数据中学习最具辨别力的特征。

为了编码用于识别的三维空间，我们建议采用一个定向的截断符号距离函数（TSDF）。给定一个三维空间，我们将其划分为一个等距的三维体素网格。<u>每个体素的值被定义为体素中心与输入深度图的表面之间的最短距离。</u>图3显示了几个例子。为了编码表面点的方向，而不是单一的距离值，我们提出了一个方向性的TSDF，在每个体素中存储一个三维矢量[dx, dy, dz]，以记录到最近的表面点的三个方向的距离。该值被剪切为2δ，其中δ是每个维度的网格大小。该值的符号表示该单元是在表面的前面还是后面。

为了进一步加快TSDF的计算，作为一个近似值，我们也可以使用投影TSDF来代替精确的TSDF，在这里只在摄像机的视线上找到最近的点。投射式TSDF的计算速度更快，但根据经验，与精确的TSDF相比，识别性能更差（见表2）。我们还对其他编码进行了实验，发现所提出的方向性TSDF优于其他所有的选择（见表2）。请注意，我们也可以通过在每个体素上附加RGB值，在这个三维体积表示法中对颜色进行编码[28]。

## 3. Multi-scale 3D Region Proposal Network

区域建议的生成：我们希望在三维中采用区域提议的方法，而不是在原始的 "滑动形状 "中进行穷举式搜索，加快计算速度，同时仍然利用三维信息。

但在三维中存在几个独特的挑战。

1. 高维度：首先，由于多了一个维度，一个物体的可能位置增加了30倍。这使得区域建议步骤变得更加重要和具有挑战性，因为它需要有更多的选择性。
2. 遮挡：其次，我们对正交检测感兴趣，其目的是估计覆盖物体全部范围的完整的三维盒子。因此，一个算法需要推断出可见部分以外的完整方框。
3. 不同大小：第三，不同的物体类别在三维中具有非常不同的物体大小。在二维中，一张图片通常只聚焦于物体的。在二维中，由于摄影偏差，图片物体边界盒的像素区域在一个非常有限的范围内。例如，一张床和一把椅子的像素区域在图片中可能是相似的，而它们的三维物理尺寸却有很大的不同。

为了应对这些挑战，我们提出了一个多尺度的三维区域建议网络（RPN），利用反向传播学习三维物体性

> 输入输出、范围和分辨率（TSDF编码）、方向（曼哈顿假设 RANSAC）、Anchor（多尺度与尺度变化）、全三维卷积架构、去除空箱、训练取样-三维箱体回归-多任务损失、三维NMS

输入/输出：我们的RPN将一个<u>三维场景作为输入</u>，并输出一组具有物体性评分的三维正交物体边界盒。该网络旨在充分利用三维物理世界的信息，如物体的尺寸、接受场的物理尺寸和房间方向。

我们的RPN不是基于自下而上的分割方法（例如[27]），它只能识别可见的部分，而是以类似于滑动窗口的方式查看整个物体的所有位置，以生成正交的物体建议。<u>为了处理不同的物体大小，我们的RPN以两种不同大小的感受器为目标。</u>

范围和分辨率：对于任何给定的三维场景，我们将其旋转以与重力方向对齐作为我们的相机坐标系。根据大多数RGB-D相机的规格，我们的目标是三维空间的有效范围：水平方向[-2.6, 2.6]米，垂直方向[-1.5, 1]米，深度[0.4, 5.6]米。在这个范围内，我们通过网格大小为0.025米的体积TSDF对三维场景进行编码，形成208×208×100的体积作为三维RPN的输入。

方向：我们希望有一个小的建议集来涵盖所有具有不同长宽比的物体。因此，作为一种启发式方法，我们建议使用房间的主要方向来确定所有建议的方向。在曼哈顿世界的假设下，我们使用RANSAC平面测定法来获得房间的方向。这种方法可以为大多数物体类别提供相当准确的边界盒方向。对于那些不遵循房间方向的物体，如椅子，它们的水平长宽比往往是一个正方形，因此，就交叉-联合而言，方向并不重要。

**Anchor：（一个位置N个Anchor）对于每个滑动窗口（即卷积）位置，该算法将预测N个区域建议在我们的案例中，根据物体大小的统计，我们确定了一组N=19个锚点，如图4所示。对于具有非正方形水平长宽比的锚点，我们设计了另一个具有相同尺寸但旋转90度的锚点。**

**Anchor变化大：我们提出了一个多尺度的RPN来输出小尺度和大尺度的建议，大尺度的RPN有一个集合层来增加大物体的接受场。我们根据锚点的物理尺寸将其分成两层，并使用网络的不同分支来预测它们。**

全三维卷积架构：为了实现三维滑动窗口式搜索，我们选择了一个全三维卷积架构。图1显示了我们的网络结构。最后一个卷积层预测物体性分数和边界盒回归的步长为1，在三维中是0.1米。滤波器的大小为2×2×2（1级）和5×5×5（2级），这相当于1级锚的接受场为0.4米^3^，2级锚为1米^3^。

![image-20210624165955541](https://oj84-1259326782.cos.ap-chengdu.myqcloud.com/uPic/2021/06_24_image-20210624165955541.png)

<u>空箱清除：考虑到范围、分辨率和网络结构，任何图像的锚点总数为1,387,646个（19×53×53×26）。平均而言，其中92.2%的锚点盒是空的，其点密度小于每厘米0.005个点3。为了避免分心，我们在训练和测试期间自动删除这些锚点。</u>

训练取样：对于剩下的锚点，如果它们的三维IOU与地面实况的分数大于0.35，我们就把它们标记为正数，如果它们的IOU小于0.15，就标记为负数。在我们的实施中，每个迷你批次包含两幅图像。我们在每幅图像中随机抽取256个锚点，正负比例为1:1。如果少于128个正样本，我们就用同一图像的负样本来填充小批。我们通过在最后的卷积层中指定每个锚的权重来选择它们。我们也尝试使用所有具有适当权重的阳性和阴性样本，但训练无法收敛。

三维箱体回归：我们用中心[c x , c y , c z]和箱体三个主要方向的箱体大小[s 1 , s 2 , s 3]来表示每个三维箱体（锚的方向为锚，人类的注释为基础事实）。为了训练三维盒子的回归器，我们将预测一个锚定盒子和它的地面真实盒子之间的中心和大小的差异。为了简单起见，我们不对方向进行回归。对于每个正的锚点和其相应的地面实况，我们用它们在相机坐标系中的差异[Δc x , Δc y , Δc z]来表示盒子中心的偏移。对于尺寸差异，我们首先找到两个盒子之间最接近的匹配的主要方向，然后计算每个匹配方向上的盒子尺寸偏移[Δs 1 , Δs 2 , Δs 3]。与[18]类似，我们通过其锚点尺寸将尺寸差异归一化。我们的三维盒子回归的目标是每个正的锚点t = [Δc x , Δc y , Δc z , Δs 1 , Δs 2 , Δs 3 ]的6元素向量。

多任务损失：按照[7, 18]中的多任务损失，对于每个锚，我们的损失函数被定义为。

L(p, p ∗ , t, t ∗ ) = L cls (p, p ∗ ) + λp ∗ L reg (t, t ∗ ) 

其中，第一项是对象性评分，第二项是盒式回归。p是这个锚点是一个对象的预测概率，p∗是地面真相（如果锚点是正面的，则为1，如果锚点是负面的，则为0）。L cls是两个类别（物体与非物体）的对数损失。第二项制定了正向锚的三维边界盒回归（当p ∗ = 1时）。L reg是平滑的L 1损失，用于Fast-RCNN[7]的二维盒子回归。

三维NMS：RPN网络为每个非空的提议框（由回归结果抵消的锚）产生一个对象性分数。为了去除多余的提议，我们在这些盒子上应用三维非最大抑制（NMS），三维的IOU阈值为0.35，并且只挑选前2000个盒子输入到物体识别网络。这2000个盒子只占所有滑动窗口的0.14%，这也是使我们的算法比原来的Sliding Shapes[25]更快的关键因素之一。

## 4 联合模态物体识别网络

<img src="https://oj84-1259326782.cos.ap-chengdu.myqcloud.com/uPic/2021/05_25_image-20210525150028770.png" alt="image-20210525150028770" style="zoom:50%;" />

鉴于三维建议框，我们将每个框内的三维空间反馈给物体识别网络（ORN）。这样一来，给ORN的最终建议可以是物体的**实际边界框**，这使得ORN可以查看完整的物体以提高识别性能，同时在计算上仍然是**高效**的。此外，由于我们的建议是包含整个物体全部范围的正交箱，ORN可以有意义地对准三维物体，以便在识别时更**不受遮挡或数据丢失的影响**。

1. 三维物体识别网络：
   1. 对于每个提议框，我们在每个方向上用12.5%的尺寸来**填充**提议框，以编码一些背景信息。
   2. 我们将空间**划分**为30×30×30的体素网格
   3. 使用TSDF（第2节）来**编码**物体的几何形状。网络结构如图2所示。所有的最大池化层都是2^3^，跨度为2。对于三个卷积层，窗口大小为5^3^、3^3^和3^3^，跨度均为1。在全连接层之间有ReLU和dropout层（dropout比率为0.5）。图5显示了5,000个前景体积的二维t-SNE嵌入，使用他们从三维ConvNet学到的最后一层特征。颜色编码物体类别。

2. 二维物体识别网络：三维网络只利用了深度图，但没有利用颜色。对于某些物体类别，颜色是一个非常有辨别力的特征，现有的ConvNets为基于图像的识别提供了非常强大的特征，可能是有用的。对于每一个三维建议框，我们将建议框内的三维点投射到二维图像平面，并得到包含所有这些二维点投射的二维框。我们使用在ImageNet[19]上预训练的最先进的VGGnet[22]（没有微调）来提取图像的颜色特征。我们使用Fast RCNN[7]中的Region-of-Interest Pooling Layer从conv5 3层中均匀采样7×7点，使用二维窗口再加上一个全连接层来生成4096维的特征，作为二维图像的特征。

我们也尝试了在三维体素上编码颜色的替代方法，但它的表现比预训练的VGGnet差得多（表2 [dxdydz+rgb] vs. [dxdydz+img]）。这可能是因为在三维体素网格中对颜色进行编码，与原始图像相比，明显降低了分辨率，因此，图像中的高频信号会丢失。此外，通过使用VGG的预训练模型，我们能够利用ImageNet的大量训练数据和精心设计的网络结构。

**二维和三维联合识别：我们构建了一个二维和三维联合网络，以利用颜色和深度。来自二维VGG网络和三维ORN的特征（每个都有4096个维度）被串联成一个特征向量，并送入一个全连接层，该层将维度减少到1000。另外两个全连接层将这个特征作为输入，并预测物体标签和三维盒子。**

多任务损失与RPN类似，损失函数由分类损失和三维方框回归损失组成。

L(p, p ∗ , t, t ∗ ) = L cls (p, p∗ ) + λ [p ∗ > 0]L reg (t, t ∗ ), (2) 其中p是对20个物体类别的预测概率（消极的非物体被标记为0类）。对于每个小批次，我们从不同的图像中抽取384个例子，正负比例为1：3。对于盒式回归，每个目标偏移量t∗都是以物体类别的具体平均值和标准偏差为元素进行归一化的。在测试过程中，我们把0.1作为3D NMS的阈值。对于盒式回归，我们直接使用网络的结果。

物体大小修剪 当我们使用模态边界盒来表示物体时，边界盒的大小提供了关于物体类别的有用信息。为了利用这些信息，对于每个检测到的盒子，我们检查每个方向上的盒子尺寸，每对盒子边缘的长宽比。然后，我们将这些数字与从同一类别的训练实例中收集的分布进行比较。如果这些数值中的任何一个落在分布的第1至99个百分点之外，这表明这个盒子的尺寸非常不同，我们就把它的分数减少2分。

## 5 实验

在NVIDIA K40 GPU上，RPN和ORN的训练分别需要大约10小时和17小时。在测试过程中，RPN需要5.62秒，ORN需要13.93秒，这比深度RCN（40秒CPU+30秒GPU+昂贵的后期对齐）和滑动形状（25分钟×物体类别数量）快得多。我们在Marvin[30]中实现我们的网络架构，这是一个支持N维卷积神经网络的深度学习框架。对于VGG网络[22]，我们使用了[12]中的权重，没有进行细微的调整。

我们在标准的NYUv2数据集[21]和SUN RGB-D[24]数据集上评估我们的三维区域建议和物体检测算法。

### 5.1. 物体建议评估

表1显示了对纽约大学数据集上的对象建议的评估。在左边，我们显示了不同IOU的平均召回率。在右边，我们显示了IOU阈值为0.25时每个物体类别的召回率，以及所有地面真实箱的平均最佳重叠率（ABO）。表中显示了对SUNRGB-D数据集的评估。

三维选择搜索：我们研究一个类似的基于自下而上分组的方法在三维（三维SS）中的效果如何。它从二维分割开始，使用分层分组来获得不同尺度的物体建议。

我们首先在三维点云上使用平面精确定位，以获得一个初始分割。对于每个占总图像面积10%以上的大平面，我们使用来自[11]的RGB-D UCM分割法（阈值为0.2）来进一步分割它。

我们的三维RPN：表1中第3至5行显示了我们的三维区域建议网络的性能。第3行显示的是单尺度RPN的性能。请注意，像灯、枕头、垃圾桶这样的小物体的召回率非常低。<u>当多加一个比例时，这些小物体的性能就会有明显的提高</u>。

在三维TSDF编码中加入RGB颜色，性能略有提高，我们将其作为我们最终的区域建议结果。从比较中我们可以看出，<u>大多数平面物体（如门）使用基于分割的选择性搜索更容易定位。</u>一些类别（如灯）的召回率较低，主要是因为缺乏训练实例。

表2显示了<u>使用相同的ORN架构但不同的建议</u>（行[3D SS: dxdydz]和行[RPN: dxdydz]）时的检测AP。我们可以看到，RPN提供的建议有助于提高检测性能，幅度很大（mAP从27.4到32.3）。

