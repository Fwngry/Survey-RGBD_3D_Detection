# Expandable YOLO

> 更快的、更少参数的系统，解决遮挡问题。
>
> Complex-YOLO[9]和YOLO3D[10]
>
> [9] M. Simon, S. Milz, K. Amende, and H. M. Gross, “Complex-YOLO: An Euler-Region-Proposal for Real-Time 3D Object Detection on Point Clouds,” in Proc. of the European Conference on Computer Vision (ECCV), pp. 197-209, 2018.
>
> [10] W. Ali, S. Abdelkarim, M. Zahran, M. Zidan, and A. E. Sallab, “YOLO3D: End-to-end real-time 3D Oriented Object Bounding Box Detection from LiDAR Point Cloud,” arXiv preprint arXiv:1808.02350, 2018.

## 摘要

本文旨在构建轻量级的检测器，它从立体相机中输入深度和彩色图像。使用距离信息作为输入以自动驾驶。然而，传统的检测器具有庞大的网络结构，实时性受到影响。具体来说，通过将YOLOv3的网络结构扩展到中间的三维空间，可以在深度方向上输出。此外，还引入了三维空间中的交叉点（IoU）来确认区域提取结果的准确性。这个实验能够输出三维边界框，并检测出身体部分被隐藏的人。此外，该模型的处理速度为44.35 fps。

## 引言

三类网络结构已经取得了重要的成果。

首先是基于区域的CNN（R-CNN）[1]类型，由Fast/Faster/Mask[2, 3, 4]R-CNN代表。其中，Mask R-CNN通过在网络中放入全卷积网络（FCN）[5]来实现实例分割。然而，这种类型的网络对每个对象都有一个卷积神经网络。由于这个原因，网络结构和计算成本都很大。

第二种是单次多盒检测器（SSD）[6]类型。由于SSD比R-CNN更快，并且反映了每个尺度的特征提取结果，所以它的优点是即使场景中存在多个物体，也能保持稳健。然而，SSD有许多任意的参数，如基本矩形的比例选择和大小设置。

最后，You Only Look Once（YOLO）[7]由一个简单的网络组成，YOLOv2[8]取得了比SSD更高的精度和处理速度。然而，由于该算法的局限性，如果物体靠近同一锚点（物体合并），就很难检测到物体是否相邻。

另一方面，在人的计数和自动驾驶中，有必要考虑物体的深度信息来检测物体，以应对物体之间的闭塞。然而，这些网络结构是巨大的，而且不是实时的。

此外，这些都只把点云作为输入，颜色信息不能被考虑在内。

还有一些强调实时特性的检测器，如Complex-YOLO[9]和YOLO3D[10]。

然而，这些都是使用鸟瞰图的点云的检测方法。因此，有必要使用另一个过程，如与主观图像相关联，并使用能够获取大范围点的传感器，如激光成像探测和测距（LiDAR）。

因此，本研究的目的是构建一个物体检测器，将深度信息和颜色信息作为输入并识别物体的三维位置。**此外，我们旨在通过减少网络的权重来构建一个更快的系统。这使得使用立体相机轻松快速地识别三维位置成为可能，而传统上这是由昂贵的测距仪（如LiDAR）完成的。**在提出的模型中，三维边界框是基于YOLO结构的输出。这可以预期，即使对于有遮挡物的场景也是稳健的，而这些遮挡物很难通过简单地结合基于二维的方法和深度图像来分离。在第二章，我们描述了网络结构和损失函数，在第三章，我们解释了使用数据集的验证实验。

为了证实所提出的模型的有效性，使用多个数据集验证了模型的准确性。此外，为了验证实时性，对所实施系统的每个部分的处理时间进行了测量。

## EXPANDABLE-YOLO

### 1. 网络结构

提出的网络结构如图1所示。我们将其命名为 "可扩展的YOLO"（以下称 "E-YOLO"）。E-YOLO在概念上很简单。YOLOv3[11]使用了一个由RGB通道组成的彩色图像作为输入；我们在此基础上增加了一个新的深度图像通道，并将其作为一个单一的图像输入。由此，该网络能够同时从彩色和深度图像中提取特征。在E-YOLO中，以这种方式完成的4after通道的图像被用作输入。

<img src="https://oj84-1259326782.cos.ap-chengdu.myqcloud.com/uPic/2021/06_19_06_19_image-20210619154412204.png" alt="image-20210619154412204" style="zoom:50%;" />

在YOLOv3中使用的Darknet-53，被作为提取特征的网络结构，一直到中间阶段。原因是在YOLOv3中，预测边界盒的特征提取是成功的，与用残差网络（ResNet）[12]进行的情况相比，在精度和处理速度上得分都很高。因此，在从二维图像中提取特征时，Darknet-53是一种有效的网络结构。

在E-YOLO中，深度信息包含在输入中，但由于这是作为深度图像的输入，我们认为可以像在彩色图像的情况下进行特征提取。

接下来，我们解释网络结构的一部分，即使用Darknet-53从特征中输出一个三维边界框。在这部分中，如图2所示，在不同的尺度上获得两个输出。这两层使用上采样进行连接，并使用1×1核滤波器进行卷积。之后，在通道方向上被分成26个通道。这使得张量成为三维方形。我们在这一部分受到统一检测的启发，这是YOLO的一个共同概念。统一检测是一个概念，通过存储每个通道的边界盒坐标和分类结果，可以同时输出分类和区域识别。基于这个概念，如果可以控制每个通道的输出值的类型，同样可以对每个通道范围进行控制。这里，YOLOv3中采用了特征金字塔网络（FPN）[13]结构；但是，所提出的模型只处理中间部分的比例，以减少计算成本。

通过这种网络结构，E-YOLO从彩色图像和深度图像中输出一个三维边界盒。如图2所示，所有卷积层都使用3×3或1×1内核滤波器。此外，输入图像的每个通道的大小为416×416，输出的大小为26×26×26，因为形状是从二维转为三维的。因此，E-YOLO可以检测图3中的场景。输入通道的数量是4个；彩色图像的R、G、B和深度。另一方面，输出通道的数量是10个。在这些通道中，前8个通道用于输出三维边界框，它们是置信值、不可靠值、边界框的锚点x、y和z，边界框的宽度、高度和深度大小。其他的是用于识别类别（人或物）。我们将分类的通道数设置为与YOLOv3的通道数相同的数值。在初步实验中，我们估计通过设置这些内核大小获得最佳性能。

![image-20210619174104936](https://oj84-1259326782.cos.ap-chengdu.myqcloud.com/uPic/2021/06_19_06_19_image-20210619174104936.png)

### 应用细节

![image-20210619174024260](https://oj84-1259326782.cos.ap-chengdu.myqcloud.com/uPic/2021/06_19_image-20210619174024260.png)

我们按照现有的YOLOv3工作设置了Darknet-53的超参数。其他参数的设置如图2所示。但为了减少计算成本，每个网格中的边界盒B的数量被设置为1

训练方法描述如下。与YOLOv3相比，所提出的模型也在深度方向上输出。因此，我们使用YOLO3D的损失函数包括深度信息的平方误差进行训练。该损失函数如下：

![image-20210619173952604](https://oj84-1259326782.cos.ap-chengdu.myqcloud.com/uPic/2021/06_19_06_19_06_19_image-20210619173952604.png)

方程（1）中几乎所有的值都与YOLO3D的值具有相同的含义。G=（高度/26）×（宽度/26）×（深度/26）的值是网格中的单元数。此外，λ表示任意设置的误差用法，对于中心坐标和包围盒尺寸的误差设置为1，对于不存在物体时的误差设置为10。变量t是组成界线盒的每个值（锚点的x、y、z，以及宽度和高度），c表示界线盒在该位置存在的信心。符号p被定义为一个变量，表示属于某个类别的概率。符号l表示监督数据中包含的边界盒是否存在于该单元格中。如果界线盒存在，则计算与界线盒和分类有关的误差，如果不存在，则计算对象的可靠性值部分。

最佳边界盒候选人的选择描述如下。一个典型的方法是非最大限度的抑制（NMS）。这是一种根据代表区域重叠程度的分数来消除对同一物体估计的界线盒的方法，称为交叉大于联合（IoU）。这里，传统的处理三维信息的物体检测器，如YOLO3D，从传感器正面和垂直方向的两个方向计算二维的IoU，并通过NMS选择边界框。然而，采用这种方法，对于同一个三维边界盒，要计算两次IoU，在GPU上进行并行计算时，效率很低。因此，在提议的模型中，定义了代表体积重叠程度的3D IoU，如图4所示，并在此基础上执行NMS。因此，IoU的计算只在GPU上执行一次，处理速度的提高是可以预期的。至于NMS中IoU的阈值，在YOLO、R-CNN和SSD中使用了0.5。但在体积比的情况下，最好使用0.35。

## 3. 实验

我们使用我们创建的数据集来训练模型，测量学习所需的时间，并使用IoU得分来评估模型的准确性。

（数据集）我们使用RealSense D435[14]在中央大学后乐园校区的2720室获取数据。如图5所示，这些数据是在室内拍摄的，当时人们正在行走。标签是由Mask R-CNN自动创建的，并手动标注了那些置信度较低的标签。深度值的标签是使用聚类法对从RGB-D生成的点云进行的。深度被归一化为10米的范围。我们使用以这种方式获得的1240个场景数据来训练模型。这些数据被分为1140条训练数据和100条验证数据。我们将学习率设置为0.001，并选择自适应矩估计（Adam）[15]作为优化方法。训练时的机器规格为GPU。NVIDIA RTX 2080和CPU。英特尔酷睿i7 8700K。训练是端到端进行的，包括darknet。

此外，我们认为，通过将特征提取器从Darknet改为其他网络，可以进一步减少误差。这是因为Darknet是为YOLOv3架构优化的，而不是为3D卷积网络优化的。所以需要用任何自动机器学习方法来优化网络的参数[16]。

图7和图8显示了YOLOv3和E-YOLO的训练模型的检测结果。使用拟议模型的检测结果是用点云库（PCL）[17]渲染的。所提出的模型能够以与YOLOv3相同的方式检测二维物体，并且还能同时估计深度方向。此外，虽然原来的YOLO没有检测到穿黑色衣服的中心人物，但我们提出的模型可以检测到这一点。然而，与gt相比，界线盒在深度方向上的精度仍然不够。

为了定量评估估计的边界盒的准确性，我们使用了IoU得分。我们还用图4中描述的3D IoU进行了评估，因为所提出的模型输出的是3D边界盒。

表二显示了训练数据的IoU得分的结果。

![image-20210619162236919](https://oj84-1259326782.cos.ap-chengdu.myqcloud.com/uPic/2021/06_19_06_19_image-20210619162236919.png)根据以前的研究，YOLOv3的IoU得分对每一类都是0.7到0.8左右。所提出的模型在二维的边界盒的准确性略有下降。这是因为提议的模型没有采用YOLOv3中使用的FPN结构。通过使用FPN结构，对不同尺寸的物体的鲁棒性得到了改善，但处理速度却降低了。因此，它有望提高精确度。

同时，考虑到处理速度，通过将其部分纳入网络来保持实时性。我们还比较了2D IoU和3D IoU的得分。乍一看，3D IoU得分似乎很低；然而，从体积比和面积比之间的关系来看，2D IoU与3D IoU 2/3对应。从这个结果来看，我们认为所提出的模型可以在相同程度上提取二维和三维特征。因此，为了提高准确性，需要对网络结构中同时执行二维卷积和三维卷积的部分进行改进，如多层化。

C. 处理速度的验证

我们验证了拟议模型的处理速度。用于验证的机器规格是GPU为GTX 1080 Ti，CPU为Core i7 4790。该模型是用PyTorch[18]实现的。结果显示在表三中。结果，尽管所提出的模型的处理速度低于YOLOv3，但它能够作为一个网络以非常高的速度运行，以获得三维输出。这是因为通过使用二维卷积层进行特征提取，减少了三维卷积层的数量。然而，由于有必要增加层数以提高物体检测的准确性，通过提高RGB-D特征提取方法的效率，在保持处理速度的同时提高了准确性。

IV. 结论

在这项研究中，我们构建了一个轻量级的网络，可以从RGB-D图像中输出一个3D边界盒。通过使用深度图像而不是点云，通过二维卷积获取深度方向的特征，有可能实现比三维处理更轻的网络。在我们未来的工作中，我们的目标是扩展所提出的网络以进行实例分割。