>  原文地址 [blog.csdn.net](https://blog.csdn.net/qq_40196164/article/details/86005211)
>
> Open source：https://github.com/charlesq34/frustum-pointnets

 Frustum PointNets
==============================================

**摘要**
========

在这项工作中，我们研究了室内和室外场景中 RGB-D 数据的三维物体检测。 虽然以前的方法专注于图像或 3D 体素，通常模糊自然 3D 图案和 3D 数据的不变性，但我们通过弹出 RGB-D 扫描直接对原始点云进行操作。 然而，这种方法的一个关键挑战是如何在大规模场景的点云中有效地定位对象（区域提议）。 我们的方法不仅仅依靠 3D 建议，而是利用成熟的 2D 物体探测器和先进的 3D 深度学习来实现物体定位，实现效率以及即使是小物体的高召回率。 受益于直接在原始点云中学习，我们的方法也能够精确估计 3D 边界框，即使在强遮挡或非常稀疏的点下也是如此。 在 KITTI 和 SUN RGB-D 3D 检测基准测试中，我们的方法在具有实时功能的同时，以惊人的利润优于最先进的技术。

**1. 介绍** 
==========

最近，在 2D 图像理解任务方面取得了很大进展，例如物体检测 [10] 和实例分割[11]。 然而，除了获得 2D 边界框或像素掩模之外，在诸如自动驾驶和增强现实（AR）的许多应用中迫切需要 3D 理解。 随着部署在移动设备和自动驾驶车辆上的 3D 传感器的普及，越来越多的 3D 数据被捕获和处理。 在这项工作中，我们研究了最重要的三维感知任务之一 - 三维物体检测，它对物体类别进行分类，并从三维传感器数据中估算物理对象的三维边界框。

虽然 3D 传感器数据通常采用点云的形式，但如何表示点云以及用于 3D 对象检测的深层网络架构仍然是一个悬而未决的问题。

大多数现有作品通过投影 [30,21] 将 3D 点云转换为图像，或通过量化 [33,18,21] 将体积网格转换为体积网格，然后应用卷积网络。然而，该数据表示变换可能模糊自然 3D 图案和数据的不变性。 最近，许多论文提出直接处理点云而不将它们转换成其他格式。 例如，[20,22]提出了新类型的深网络体系结构，称为 PointNets，它在几个 3D 理解任务中表现出卓越的性能和效率，如对象分类和语义分割。虽然 PointNets 能够对点云进行分类或预测点云中每个点的语义类，但尚不清楚该架构如何用于实例级 3D 对象检测。

**为实现这一目标，我们必须解决一个关键挑战：如何在 3D 空间中有效地提出 3D 对象的可能位置。**模仿图像检测中的实践，通过滑动窗口 [7] 或通过 3D 区域提议网络（如[27]）枚举候选 3D 框是直截了当的。然而，3D 搜索的计算复杂度通常在分辨率方面立方体地增长，并且对于大型场景或诸如自动驾驶的实时应用而言变得太昂贵。

<u>相反，在这项工作中，我们按照降维原理缩小搜索空间：我们利用成熟的 2D 物体探测器（图 1）。</u>

**（2D proposal - 3D frustum）**首先，我们通过从图像检测器中挤出 2D 边界框来提取对象的 3D 边界平截头体。

**（调整 - 实例分割+回归）**然后，在由每个 3D 平截头体修剪的 3D 空间内，我们使用 PointNet 的两个变体连续地执行 3D 对象实例分割和非模态3D 边界框回归。分割网络预测感兴趣对象的 3D 掩模（即实例分割）; 并且回归网络估计了非模态的 3D 边界框（覆盖整个对象，即使只有部分可见）。

**（3D中心的处理）**与之前将 RGB-D 数据视为 CNN 的 2D 地图的工作相比，我们的方法更加以 3D 为中心，因为我们将深度图提升为 3D 点云并使用 3D 工具处理它们。这种以 3D 为中心的视图实现了以更有效的方式探索 3D 数据。

**（对齐）**首先，在我们的pipeline中，在 3D 坐标上连续应用了一些将点云对齐成一系列更受约束和规范的帧的变换。这些对齐分解出数据中的姿势变化，从而使 3D 几何图案更加明显，3D 学习者的工作更容易。

其次，在三维空间中学习可以更好地利用三维空间的几何和拓扑结构。原则上，所有物体都存在于 3D 空间中; 因此，我们相信许多几何结构，例如重复，平面性和对称性，更自然地被直接在 3D 空间中操作的学习者参数化和捕获。最近的实验证据支持这种以 3D 为中心的网络设计理念的有用性。

我们的方法在 KITTI 3D 物体检测 [1] 和鸟瞰图检测 [2] 基准测试中取得了领先地位。 与以前的技术水平相比 [5]，我们的方法在 3D 汽车 AP 上效率高达 8.04％，效率高（以 5 fps 运行）。 我们的方法也非常适合室内 RGBD 数据，我们在 SUN-RGBD 上比[13] 和[24]实现了 8.9％和 6.4％更好的 3D mAP，同时运行速度提高了一到三个数量级。

2. 相关工作
===========

**RGB-D 数据的 3D 物体检测**

研究人员通过各种方式表示 RGB-D 数据，从而解决了 3D 检测问题。

1. 基于前视图图像的方法

[3,19,34] 采用单眼 RGB 图像和形状先验或遮挡模式来推断 3D 边界框。 

[15,6] 将深度数据表示为 **2D 地图**并应用 CNN 来定位 2D 图像中的对象。 

**将深度表示为点云**，并使用可以更有效地利用 3D 几何的高级 3D 深度网络（PointNets）。

2. 基于鸟瞰图的方法

MV3D [5] 将 LiDAR 点云投射到鸟瞰图并训练区域建议网络（RPN [23]）用于 3D 边界框提议。 然而，该方法在检测诸如行人和骑车者的小物体方面滞后，并且不能容易地适应具有垂直方向上的多个物体的场景。

3. 基于 3D 的方法

[31,28]通过 SVM 在由点云提取的手工设计的几何特征上训练 3D 对象分类器，然后使用滑动窗口搜索来定位对象。

[7]通过在体素化 3D 网格上用 3D CNN 替换 SVM 来扩展 [31]。 

[24] 设计了点云中三维物体检测的新几何特征。

<u>[29,14]将整个场景的点云转换为体积网格，并使用 3D 体积 CNN 进行对象建议和分类。由于 3D 卷积和大型 3D 搜索空间的昂贵成本，这些方法的计算成本通常很高。</u>

<u>[13]提出了一种 2D 驱动 3D 物体检测方法，它与我们的精神相似。</u>

**结论：他们使用手工制作的功能（基于点坐标的直方图）和简单的FCN来回归 3D 盒子位置和姿势，这在速度和性能方面都是次优的。**

相比之下，我们提出了一种更灵活，更有效的深度 **3D 特征学习解决方案（PointNets）**。

**点云深度学习**

（体素化+3D CNN）大多数现有作品在特征学习之前将点云转换为图像或体积形式。 [33,18,21]将点云体素化为体积网格，并将图像 CNN 概括为 3D CNN。[16,25,32,7]设计更有效的 3D CNN 或神经网络架构，利用点云中的稀疏性。 然而，这些基于 CNN 的方法仍然需要具有特定体素分辨率的点云的量化。

（PointNets）最近，一些作品 [20,22] 提出了一种新型网络架构（PointNets），它直接消耗原始点云而不将它们转换为其他格式。 虽然 PointNets 已应用于单个对象分类和语义分割，但我们的工作探索了如何扩展体系结构以实现 3D 对象检测。

问题定义
===========

给定 RGB-D 数据作为输入，我们的目标是在 3D 空间中对对象进行分类和本地化。 从 LiDAR 或室内深度传感器获得的深度数据表示为 RGB 相机坐标中的点云。 投影矩阵也是已知的，以便我们可以从 2D 图像区域获得 3D 平截头体。每个对象由一个类（k 个预定义类中的一个）和一个 amodal 3D 边界框表示。即使对象的一部分被遮挡或截断，amodal 框也会绑定整个对象。 3D 框通过其尺寸 h，w，l，中心![](https://private.codecogs.com/gif.latex?c_x)，![](https://private.codecogs.com/gif.latex?c_y)，![](https://private.codecogs.com/gif.latex?c_z)和相对于每个类别的预定规范姿势的方向θ，φ，ψ来参数化。 在我们的实现中，我们仅考虑围绕上轴的方向角θ以进行定向。

3D 检测使用 Frustum PointNets
================================

![](https://img-blog.csdnimg.cn/20190110093701327.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMTk2MTY0,size_16,color_FFFFFF,t_70)

1. 我们首先利用 2D CNN 物体探测器来提出 2D 区域并对其内容进行分类。 然后将 2D 区域提升到 3D，从而成为截头的提议。 
2. 给定平截头体中的点云（n×c，具有 n 个点和 x 个通道的 XYZ，每个点的强度等），通过每个点的<u>二元分类来对对象实例进行分段</u>。 基于分割的对象点云（m×c），轻量级回归 PointNet（T-Net）尝试**通过平移对齐点**，使得它们的质心接近于 amodal box center。
3. box estimation pointnet 估计对象的 amodal 3D 边界框。 有关坐标系和网络输入，输出的更多说明见图 4 和图 5。

平截头体提议，三维实例分割和三维模块边界框估计。 我们将在以下小节中介绍每个模块。 我们将重点关注每个模块的管道和功能，并将读者引用到所涉及的深度网络的特定体系结构的补充。

4.1 Frustum Proposal
--------------

大多数 3D 传感器（尤其是实时深度传感器）产生的数据分辨率仍低于商用相机的 RGB 图像。 因此，我们利用成熟的 2D 物体探测器来提出 RGB 图像中的 2D 对象区域以及对对象进行分类。

利用已知的相机投影矩阵，可以将 2D 边界框提升到平截头体（具有由深度传感器范围指定的近和远平面），其定义对象的 3D 搜索空间。 然后我们收集平截头体内的所有点以形成平截头体点云。 如图 4（a）所示，平截头体可以朝向许多不同的方向，这导致点云的放置的大的变化。 因此，我们通过将平截头体朝向中心视图旋转来使截头锥体归一化，使得平截头体的中心轴线与图像平面正交。 此归一化有助于改善算法的旋转不变性。 我们将这整个过程称为从 RGB-D 数据平截头提议生成中提取平截头体点云。

虽然我们的 3D 检测框架与 2D 区域提议的确切方法无关，但我们采用基于 FPN [17] 的模型。 我们在 ImageNet 分类和 COCO 对象检测数据集上预先训练模型权重，并在 KITTI 2D 对象检测数据集上进一步微调它以对 amodal 2D 框进行分类和预测。 补充中提供了 2D 探测器训练的更多细节。

4.2 3D 实例分割
-----------

给定 2D 图像区域（及其对应的 3D 平截头体），可以使用几种方法来获得对象的 3D 位置：

1. （原方案：2D图像回归3D对象位置 - 问题：遮挡）一种直接的解决方案是使用 2D 从深度图直接回归 3D 对象位置（例如，通过 3D 边界框）。 然而，这个问题并不容易，因为遮挡物体和背景杂乱在自然场景中很常见（如图 3 所示），这可能会严重分散 3D 定位任务的注意力。 

2. （解决方案：点云分割）因为物体在物理空间中是自然分离的，所以 3D 点云中的分割比来自远处物体的像素彼此接近的图像中的分割更加自然和容易。 观察到这一事实后，我们建议在 3D 点云中而不是在 2D 图像或深度图中分割实例。我们使用基于 PointNet 的网络在截头锥体上的点云实现 3D 实例分割.

   基于 3D 实例分割，我们能够实现基于残差的 3D 定位。 也就是说，不是回归物体的绝对 3D 位置，其偏离传感器的偏差可能在很大的范围内变化（例如在 KITTI 数据中从 5 米到超过 50 米），我们预测局部坐标系中的 3D 边界框中心 -  3D 蒙版 坐标如图 4（c）所示。

<img src="https://img-blog.csdnimg.cn/20190110111320272.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMTk2MTY0,size_16,color_FFFFFF,t_70" style="zoom: 67%;" />

> 图 3. 截锥点云中 3D 检测的挑战。 左图：带有图像区域建议的 RGB 图像。 右图：从 2D 框中挤出平截头体中的 LiDAR 点的鸟瞰图，在那里我们看到了前景遮挡（自行车）和背景杂乱（建筑物）的广泛点。

**3D 实例分割 PointNet** 

（总思路；概率表示感兴趣对象）网络在平截头体中采用点云并且预测每个点的概率分数，其指示该点属于感兴趣对象的可能性。 请注意，每个视锥体仅包含一个感兴趣的对象。 这些 “其他” 点可以是非相关区域（例如地面，植被）或其他遮挡或位于感兴趣对象后面的点。

（步骤1 - 实例分割）我们的分割 PointNet 正在学习**遮挡和杂乱**模式以及识别某个类别的几何对象。在多类检测案例中，我们还利用 2D 检测器的语义来实现更好的实例分割。 例如，如果我们知道感兴趣的对象是行人，那么分割网络可以在找到看起来像人的几何之前使用它。 具体来说，在我们的体系结构中，我们将语义类别编码为单热类向量（预定义 k 类的 k 维），并将单热矢量连接到中间点云特征。 补充说明了具体体系结构的更多细节。

（步骤2 - 提取感兴趣点）在 3D 实例分割之后，提取被分类为感兴趣对象的点（图 2 中的 “掩蔽”）。

（步骤3 - 标准化坐标，增强平移不变性）随后获得这些分割的对象点，我们进一步标准化其坐标以增强算法的平移不变性，遵循相同的在截头提案步骤中的基本原理。在我们的实现中，我们通过按质心减去 XYZ 值，将点云转换为局部坐标。 这在图 4（c）中示出。 

请注意，我们故意不缩放点云：因为部分点云的边界球大小可能会受到视点的极大影响，并且**点云的实际大小有助于估计框大小。**

在我们的实验中，我们发现坐标变换（例如上面的一个和前一个平截头体旋转）对于 3D 检测结果至关重要，如表 8 所示。

<img src="https://img-blog.csdnimg.cn/20190110155029284.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMTk2MTY0,size_16,color_FFFFFF,t_70" style="zoom:50%;" />

**图 4. 点云的坐标系**。 人工点（黑点）显示为

（a）默认摄像机坐标; 

（b）将截头锥体<u>旋转到中心视图</u>后的平截头坐标（第 4.1 节）; 

（c）掩饰与原点处物点的<u>质心的坐标</u>（第 4.2 节）; 

（d）由 T-Net（轻量级pointnet）预测的物体坐标（第 4.3 节）

<img src="https://oj84-1259326782.cos.ap-chengdu.myqcloud.com/uPic/2021/05_27_image-20210527112253244.png" alt="image-20210527112253244" style="zoom:50%;" />

**图 5. PointNets 的基本体系结构和 IO**：中心点的估计+对象坐标的估计

 为具有集合抽象层和特征传播层（用于分段）的 PointNet ++ [22]（v2）模型说明了体系结构。

4.3 非模态 3D Box 估计
-----------------

给定分割的对象点（在 3D 掩模坐标中），该模块通过使用<u>框回归 PointNet</u> 和<u>预处理变换器网络T-NET</u>来估计对象的面向 amodal 的 3D 边界框。

**T-NET**

（T-NET 3D对齐）通过 T-Net 进行基于学习的 3D 对齐即使我们根据质心位置对齐了分割的对象点，我们发现掩模坐标框架的原点（图4c）可能距离模块框还很远中央。 因此，我们建议使用轻量级回归 PointNet（T-Net）来估计整个物体的真实中心，然后变换坐标，使预测的中心成为原点（图 4d）我们的 T-Net 的架构和训练类似于 [20] 中的 T-Net，它可以被认为是一种特殊类型的空间变换器网络（STN）[12]。 然而，与原始 STN 不同，它没有直接监督转换，我们明确监督我们的变换网络，以预测从掩模坐标原点到实际对象中心的中心残差。

**Amodal 3D Box Estimation PointNet** 

对于给定 3D 对象坐标中的对象点云的对象，<u>框估计网络</u>预测模式边界框（对于整个对象，即使其中一部分是看不见的）（图 4（d））。 网络体系结构类似于对象分类 [20,22]，但输出不再是对象类分数，而是 3D 边界框的参数。

如第三部分所述，我们通过其中心（![](https://private.codecogs.com/gif.latex?c_x%2Cc_y%2Cc_z)），尺寸（h，w，l）和航向角θ（沿着上轴）来参数化 3D 边界框。 我们采用 "残差" 方法进行盒子中心估计。 由盒估计网络预测的中心残差与来自 T-Net 和掩蔽点的质心的先前中心残差组合以恢复绝对中心（等式 1）。 

对于盒子大小和航向角度，我们遵循以前的工作 [23,19] 并使用分类和回归公式的混合。 具体来说，我们预先定义 <u>NS 尺寸模板和 NH 等分割角度</u>。 我们的模型将大小 / 航向（大小的 NS 分数，航向的 NH 分数）分类为预定义的类别，并预测每个类别的残差数量（ 3×NS 残余尺寸高度，宽度，长度，NH 残余角度）。 最后，净输出总共为 3 + 4×NS + 2×NH 数。 

<img src="https://oj84-1259326782.cos.ap-chengdu.myqcloud.com/uPic/2021/05_27_image-20210527142201695.png" alt="image-20210527142201695" style="zoom:50%;" />

4.4 多任务损失的训练
------------

我们同时优化所涉及的三个网络（3D 实例分割 PointNet，T-Net 和 amodal 盒估计 PointNet），具有多任务损失（如公式 2 所示）。

 ![](https://private.codecogs.com/gif.latex?L_%7Bc1-reg%7D)用于 T-Net，![](https://private.codecogs.com/gif.latex?L_%7Bc2-reg%7D) 用于箱估计网的中心回归。 ![](https://private.codecogs.com/gif.latex?L_%7Bh-cls%7D)和![](https://private.codecogs.com/gif.latex?L_%7Bh-reg%7D) 是航向角预测的损失，而![](https://private.codecogs.com/gif.latex?L_%7Bs-cls%7D)和![](https://private.codecogs.com/gif.latex?L_%7Bs-reg%7D)是箱尺寸。 Softmax 用于所有分类任务，smooth-![](https://private.codecogs.com/gif.latex?l_%7B1%7D)（huber)损失用于所有回归情况。

<img src="https://oj84-1259326782.cos.ap-chengdu.myqcloud.com/uPic/2021/05_27_image-20210527142008432.png" alt="image-20210527142008432" style="zoom:50%;" />

**用于盒子参数联合优化的拐角损失**

虽然我们的三维包围盒参数化是紧凑和完整的，但学习并不是为了最终的三维盒子精度而优化的：中心、尺寸和航向有单独的损失项。

如果中心和尺寸被准确预测，但航向角度不对，那么与gtbox的3D IoU就会被角度误差所支配。理想情况下，所有三个项（中心、尺寸、航向）都应该被联合优化，以获得最佳的三维盒子估计（在IoU度量下）。为了解决这个问题，我们提出了一个新的正则化损失，即角损失。

<img src="https://oj84-1259326782.cos.ap-chengdu.myqcloud.com/uPic/2021/05_27_image-20210527142106378.png" alt="image-20210527142106378" style="zoom:50%;" />

从本质上讲，角损失是预测盒和gtbox的八个角之间的距离之和。由于角的位置是由中心、大小和方向共同决定的，角损失能够使这些参数的多任务训练正规化。（角损失 - 中心、大小、方向）

为了计算角损失，我们首先从所有的 size templates and heading angle bins 中构建NS×NH "锚"盒。锚箱被翻译成估计的箱中心。我们把锚箱的角表示为P(k,j,i)，其中i、j、k分别是size class, heading class, and (predeﬁned) corner order。为了避免从翻转的航向估计中获得巨大的惩罚，我们进一步计算从翻转的gtbox到角（Pk）的距离，并使用原始和翻转情况的最小值。δ ij是一个二维掩码，用于选择我们关心的距离项；对于gt size/heading来说是1，其他情况下是0。

**5. 实验**
=========

实验分为三个部分 1。 首先，我们在 KITTI [8] 和 SUN-RGBD [27]（第 5.1 节）上与最先进的 3D 物体检测方法进行比较。 其次，我们提供深入分析以验证我们的设计选择（第 5.2 节）。 最后，我们展示定性结果，并讨论我们的方法的优势和局限性（第 5.3 节）。

5.1 与最先进的方法相比
-------------

我们在 KITTI [9] 和 SUN-RGBD [27] 三维物体检测基准上评估我们的 3D 物体探测器。 在这两项任务中，与最先进的方法相比，我们取得了明显更好的结果。

**KITTI**

表 1 显示了我们的 3D 探测器在 KITTI 测试装置上的性能。 我们大大优于以前的 theart-ofart 方法。 虽然 MV3D [5] 使用多视图特征聚合和复杂的多传感器融合策略，但我们基于 PointNet [20]（v1）和 PointNet ++ [22]（v2）骨干的方法在设计上更加清晰。 虽然超出了这项工作的范围，但我们希望传感器融合（特别是用于 3D 检测的图像特征聚合）可以进一步改善我们的结果。

我们还在 Tab2 中显示了我们方法在 3D 对象定位（**鸟瞰图**）上的表现。 在 3D 定位任务中，边界框被投影到鸟瞰视图平面，IoU 在定向 2D 框上进行评估。 同样，我们的方法明显优于以前的工作。（包括在投影的 LiDAR 图像上使用 CNN 的 DoBEM [35] 和 MV3D [5]，以及在体素化点云上使用 3D CNN 的 3D FCN [14]）

<img src="https://oj84-1259326782.cos.ap-chengdu.myqcloud.com/uPic/2021/05_28_image-20210528102508358.png" alt="image-20210528102508358" style="zoom:50%;" />

我们的网络输出在图6中显示，即使在非常具有挑战性的情况下，我们也能观察到准确的三维实例分割和盒子预测。

![image-20210528103333329](https://oj84-1259326782.cos.ap-chengdu.myqcloud.com/uPic/2021/05_28_05_28_image-20210528103333329.png)

我们还在表3和表4中报告了KITTI值集的性能Tab.3和Tab.4（针对汽车），以支持与更多发表的作品进行比较，并在Tab. 5（行人和骑自行车的人）以供参考。

<img src="https://oj84-1259326782.cos.ap-chengdu.myqcloud.com/uPic/2021/05_28_image-20210528103504156.png" alt="image-20210528103504156" style="zoom:33%;" />

**SUN-RGBD**

以前的大多数 3D 探测工作都专注于室外 LiDAR 扫描，其中物体在空间中很好地分离，点云是稀疏的（这使得它可以用于鸟瞰投影），或者是室内深度图，它是具有密集像素的常规图像这样的值可以容易地应用图像 CNN。 

然而，设计用于<u>鸟瞰的方法</u>可能对于在垂直空间中通常一起存在<u>多个物体的室内房间而言是不可能的</u>。 另一方面，室内聚焦方法可能<u>难以应用于 LiDAR 扫描的稀疏和大规模点云</u>。

相比之下，我们基于FRUSTEM的 PointNet 是<u>户外和室内 3D 物体检测的通用框架</u>。 通过应用我们用于 KITTI 数据集的相同管道，我们在 SUNR GBD 基准测试（表 6）上实现了最先进的性能，具有显着更高的 mAP 以及更快（10x-1000x）的推理速度。



5.2 结构设计分析
----------

在本节中，我们提供分析和消融实验来验证我们的设计选择。

**实验设置** 除非另有说明，否则本节中的所有实验均基于我们在 KITTI 数据上的 v1 模型，使用 [5] 中的 train / val split。为了分解 2D 探测器的影响，我们使用地面真实 2D 框用于区域提议，并使用 3D 框估计精度（IoU阈值0.7）作为评估指标。 我们只关注具有最多培训示例的汽车类别。

<img src="https://oj84-1259326782.cos.ap-chengdu.myqcloud.com/uPic/2021/05_28_image-20210528110728859.png" alt="image-20210528110728859" style="zoom:50%;" />

**与三维检测的替代方法相比**

在这一部分中，我们使用 2D Masks 评估了一些基于 CNN 的基线方法以及我们管道的消融版本和变体。

<img src="https://oj84-1259326782.cos.ap-chengdu.myqcloud.com/uPic/2021/05_28_image-20210528115542045.png" alt="image-20210528115542045" style="zoom:50%;" />

**1. 我们显示来自两个基于 CNN 的网络的 3D 盒估计结果。**（1）基线方法在 RGB-D 图像的gtbox上训练 <u>VGG</u> [26] 模型，并采用<u>相同的框参数和损失函数</u>作为我们的主要方法。虽然第一行中的模型直接估计来自vanilla RGB-D图像块的盒位置和参数（2）但另一个（第二行）使用从 COCO 数据集训练的 FCN 进行 2D 掩模估计（如 MaskRCNN [11] 中所示）和 仅使用蒙版区域中的要素进行预测。还通过减去 2D 掩模内的中值深度来转换深度值。 <u>然而，与我们的主要方法相比，两个 CNN 基线都得到了更糟糕的结果。</u>

解释：为了理解 CNN 基线表现不佳的原因，我们在图 7 中可视化典型的 2D 掩模预测。<u>虽然估计的 2D 掩模在 RGB 图像上以高质量出现，但 2D 掩模中仍然存在大量杂波和前景点。</u> 相比之下，我们的 3D 实例分割得到了更加清晰的结果，这大大简化了下一个模块的精细定位和边界框回归。

**2. 第三行，我们尝试了一个没有 3D 实例分割模块的Frustum-PointNet 的消融版本。** 毫不奇怪，该模型比我们的主要方法得到更糟糕的结果，这表明我们的 3D 实例分割模块的关键效果。 

**3. 第四行，我们使用来自 2D  Masks深度图（图 7）的点云来代替 3D 分割，用于 3D 盒估计。** 但是，由于 2D 蒙版无法干净地分割 3D 对象，因此性能比 3D 分割（第五行中的主要方法）差 12％以上。 

**4. 2D 和 3D 蒙版的组合使用** - 从 2D 蒙版深度图在点云上应用 3D 分割 - 也显示比我们的主要方法稍差的结果可能是由于来自不准确的 2D 蒙版预测的累积误差。

**点云归一化的影响** 

我们的视锥体 PointNet 采用一些关键的坐标转换来规范化点云，以便更有效地学习。Tab 8 示出了每个归一化步骤如何帮助 3D 检测。 

- Frustum rot（使得平截头体点具有更相似的 XYZ 分布）

- Mask centralize（使得对象点具有更小且更规范的 XYZ）是关键的。 

- T-Net 对象点云与对象中心的额外对齐也极大地提高了性能。

**回归损失公式和角损失的影响** 

在表 9 中：我们比较了不同的损失选项，并表明 <u>*“cls-reg” 损失 cls-reg (normalized)*</u> 和 *<u>正规角损失corner loss</u>* 的组合可以获得最佳结果。

<img src="https://oj84-1259326782.cos.ap-chengdu.myqcloud.com/uPic/2021/05_28_image-20210528120507081.png" alt="image-20210528120507081" style="zoom:50%;" />

5.3 定性结果和讨论
-----------

在图6中，我们可以看到我们的点网模型的代表性输出。

1. 我们看到，对于距离合理的非包容物体的简单情况（因此我们得到了足够多的点），我们的模型输出了<u>非常准确的三维实例分割掩码和三维边界盒</u>。

2. 我们惊讶地发现，我们的模型甚至可以从部分数据（如平行停放的汽车）中预测出正确摆放的正态三维框，而这些<u>数据的点数很少</u>。即使是人类，也很难用点云数据来注释这种结果。
3. 在某些情况下，在有很多附近甚至重叠的二维盒子的图像中似乎非常具有挑战性，当转换到三维空间时，<u>定位变得更加容易</u>。

问题

第一个常见的错误是由于在**稀疏的点云**（有时不到5个点）中对姿势和尺寸的估计不准确。我们认为图像特征可以极大地帮助我们，因为我们可以获得高分辨率的图像补丁，即使是对遥远的物体。

第二类挑战是，当一个frustum中存在同一类别的<u>多个实例</u>时（如两个人站在一起）。由于我们目前的管道假设每个Frustum中只有一个感兴趣的物体，当多个实例出现时，它可能会感到困惑，从而输出混合的分割结果。如果我们能够在每**个Frustum中提出多个三维边界盒**，这个问题就有可能得到缓解。

第三，有时我们的二维检测器会因为黑暗的光线或强烈的遮挡而漏掉物体。由于我们的Frustum建议是基于区域建议的。**在没有二维检测的情况下，将不会检测到三维物体。我们的三维实例分割和正交三维盒子估计PointNets并不限于RGB视图提议。**

如补充文件所示，同样的框架也可以扩展到鸟瞰图中提出的三维区域。

