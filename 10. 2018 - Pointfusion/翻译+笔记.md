# PointFusion

点云+RGB：简单而通用的传感器融合方法

## 摘要

我们提出了PointFusion，一种通用的三维物体检测方法，它同时利用了图像和三维点云信息。与现有的使用多阶段管道或持有传感器和数据集特定假设的方法不同，PointFusion在概念上很简单，而且与应用无关。

**图像数据和原始点云数据分别由一个CNN和一个PointNet架构独立处理。**

**由此产生的输出由一个新的融合网络结合，该网络使用输入的三维点作为空间锚，预测多个三维盒子的假设及其一致性。**

我们在两个不同的数据集上评估了PointFusion：KITTI数据集，其特点是用激光雷达-摄像机设置捕捉驾驶场景；SUN-RGBD数据集，用RGB-D摄像机捕捉室内环境。我们的模型是第一个能够在这些不同的数据集上表现得更好或与最先进的模型持平的模型，而不需要任何数据集特定的模型调整。

![image-20210626213522542](https://oj84-1259326782.cos.ap-chengdu.myqcloud.com/uPic/2021/06_26_image-20210626213522542.png)

密集的PointFusion架构的概述。PointFusion有两个特征提取器：一个是处理原始点云数据的PointNet变体（A），一个是从输入图像中提取视觉特征的CNN（B）。我们提出了两种融合网络方案：一种是直接回归盒角位置的虚构全局架构（D），另一种是预测8个角相对于输入点的空间偏移的新颖密集架构，如（C）所示：对于每个输入点，网络预测从一个角（红点）到输入点（蓝色）的空间偏移（白色箭头），并选择分数最高的预测作为最终预测（E）。

## 一、引言

我们专注于三维物体检测，这是一个基本的计算机视觉问题，影响着大多数自主机器人系统，包括自动驾驶汽车和无人机。三维物体检测的目标是恢复场景中所有感兴趣的物体的6DoF姿态和三维包围盒尺寸。虽然卷积神经网络的最新进展使复杂环境下的精确二维检测成为可能[25, 22, 19]，但三维物体检测问题仍然是一个开放的挑战。从单一图像中进行三维方框回归的方法，甚至包括最近的深度学习方法，如[21，36]，仍然有相对较低的准确性，特别是在较长距离的深度估计。因此，目前许多现实世界的系统要么使用立体声，要么用激光雷达和雷达增强其传感器堆栈。激光雷达-雷达混合传感器设置在自动驾驶汽车中特别流行，通常由一个多阶段管道处理，它分别预处理每个传感器模式，然后使用专家设计的跟踪系统（如卡尔曼仪）执行后期融合或决策级融合步骤[4, 7]。这样的系统会做出简化的假设，并在没有其他传感器的背景下做出决定。受深度学习处理各种原始感官输入的成功启发，我们提出了一个早期融合模型，用于3D箱体估计，它直接学习将图像和深度信息进行最佳组合。各种相机和三维传感器的组合在该领域被广泛使用，因此，最好能有一个单一的算法，尽可能地适用于许多不同的问题设置。许多现实世界的机器人系统都配备了多个三维传感器：例如，自动驾驶汽车通常有多个激光雷达，也可能还有雷达。然而，目前的算法往往假设只有一个RGB-D相机[32, 16]，它提供RGB-D图像，或者只有一个激光雷达传感器[3, 18]，它允许创建一个激光雷达深度和强度读数的局部前视图图像。许多现有的算法也做出了强烈的领域特定的假设。例如，MV3D[3]假设所有物体都可以在点云的自上而下的二维视图中被分割，这对常见的自驾游情况是可行的，但不能推广到室内场景，因为在室内场景中，物体可能被放在彼此的顶部。此外，自上而下的视图方法往往只对汽车等物体有效，而对其他关键的物体类别如行人或骑自行车的人则无效。与上述方法不同，我们提出的融合架构被设计成与领域无关，与三维传感器的位置、类型和数量无关。因此，它是通用的，可以用于各种机器人应用。

在设计这样一个通用模型时，我们需要解决结合异质图像和三维点云数据的挑战。以前的工作通过直接将点云转换为卷积友好的形式来解决这一挑战。这包括将点云投影到图像上[11]或将点云体素化[32, 17]。这两种操作都涉及到有损的数据量化，需要特殊的模型来处理激光雷达图像[34]或体素空间[27]中的稀疏性。相反，我们的解决方案保留了输入的原始表示，并使用异质网络架构来处理它们。具体到点云，我们使用了最近提出的PointNet[23]架构的一个变种，它允许我们直接处理原始点。

我们用于从图像和稀疏点云中进行三维物体框回归的深度网络有三个主要部分：一个现成的CNN[13]，从输入的RGB图像作物中提取外观和几何特征，一个处理原始三维点云的PointNet[23]的变体，以及一个融合子网络，结合两个输出来预测三维边界框。如图2所示，这种异质网络结构充分利用了两个数据源，而没有引入任何数据处理的偏差。我们的融合子网络有一个新颖的密集三维盒子预测架构，对于每个输入的三维点，网络预测相对于该点的三维盒子的角落位置。然后，该网络使用一个学习的评分函数来选择最佳预测。该方法受到空间锚的概念[25]和密集预测[15]的启发。其直觉是，使用输入的三维点作为锚来预测相对空间位置，与直接回归每个角的三维位置的结构相比，减少了回归目标的方差。我们证明了密集预测架构比直接回归三维角位置的架构有很大的优势。

我们在两个独特的三维物体检测数据集上评估我们的模型。KITTI数据集[10]专注于户外城市驾驶场景，其中行人、骑自行车的人和汽车被标注在用摄像机系统获得的数据中。SUN-RGBD数据集[30]是通过RGB-D摄像机在室内环境中记录的，有700多个物体类别。我们表明，通过将PointFusion与现成的二维物体检测器[25]相结合，我们得到了与为KITTI[3]和SUNRGBD[16, 32, 26]设计的最先进的方法相当或更好的三维物体检测结果。就我们所知，我们的

模型是第一个在这些非常不同的数据集上取得竞争性结果的模型，证明了其普遍适用性。

## 二、相关工作

我们概述了以前关于6-DoF物体姿态估计的工作，这与我们的方法有关。

基于几何学的方法：许多方法侧重于从单一图像或图像序列中估计6维物体的姿势。这些方法包括二维图像和其相应的三维CAD模型之间的关键点匹配[1, 5, 37]，或者将三维重建的模型与地面真实模型对齐以恢复物体的姿势[28, 9]。Gupta等人[12]提出使用CNN预测语义分割图以及物体姿势假设，然后使用ICP将假设与已知的物体CAD模型对齐。这些类型的方法依赖于强大的类别形状先验或地面真实的物体CAD模型，这使得它们难以扩展到更大的数据集。相反，我们的通用方法在没有物体类别知识或CAD模型的情况下估计物体的6-DoF姿态和空间尺寸。

来自图像的三维方块回归：深度模型的最新进展极大地改善了二维物体检测，一些方法建议用完整的三维物体姿势来扩展目标。[33]使用R-CNN提出2D RoI和另一个网络来回归物体姿势。[21]结合一组深度学习的三维物体参数和来自二维RoI的几何约束来恢复全三维盒子。

Xiang等人[36, 35]通过对从物体模型中学习到的三维体素模式进行聚类，共同学习一个与视点相关的检测器和一个姿势估计器。尽管这些方法擅长估计物体的方向，但从图像中定位物体通常是通过施加几何约束来处理的[21]，并且由于缺乏直接的深度测量，仍然是一个挑战。我们的模型的主要贡献之一是，它学会了有效地结合互补的图像和深度传感器信息。来自深度数据的3D盒子回归 较新的研究提出直接解决离散3D空间中的3D物体检测问题。

Song等人[31]利用综合生成的三维特征，学习对三维滑动窗口生成的三维边界框建议进行分类。一项后续研究[32]使用区域提议网络[25]的三维变体来生成三维提议，并使用三维ConvNet来处理体素化的点云。

Li等人[17]的类似方法专注于检测车辆，并使用三维全卷积网络处理体素化输入。然而，由于离散的体积表示法，这些方法往往是非常昂贵的。例如，DSS[32]处理一帧需要20秒左右。

其他方法，如VeloFCN[18]，专注于单一的激光雷达设置并形成密集的深度和强度图像，用单一的二维CNN进行处理。

与这些方法不同，我们采用最近提出的PointNet[23]来处理原始点云。这种设置可以容纳多个深度传感器，而且时间复杂度与测距的数量呈线性关系，而不考虑三维场景的空间范围。

2D-3D融合：我们的论文与最近融合图像和激光雷达数据的方法最为相关。Chen等人的MV3D[3]在<u>自上而下</u>的激光雷达视图中生成物体检测建议，并将其<u>投射到前面的激光雷达和图像视图</u>中，融合所有相应的特征来做定向盒回归。这种方法假设了一个单一的激光雷达设置，并加入了一个限制性的假设，即所有物体都在同一个空间平面上，并且可以仅从点云的自上而下的视图中进行定位，这对汽车有效，但对行人和骑自行车的人无效。相比之下，我们的方法没有场景或物体的特定限制，也没有对使用的深度传感器的种类和数量的限制。

## 三、PointFusion

在这一节中，我们描述了我们的PointFusion模型，它从二维图像裁剪和相应的三维点云（通常由激光雷达传感器产生）进行三维边界框回归（见图1）。当我们的模型与提供二维物体裁剪的最先进的二维物体检测器相结合时，如[25]，我们得到一个完整的三维物体检测系统。

我们把理论上直接的端到端模型留给未来的工作，因为我们已经用这种较简单的两阶段设置得到了最先进的结果。

此外，目前的设置允许我们在不修改融合网络的情况下插入任何先进的检测器。

PointFusion有三个主要组成部分：一个提取点云特征的PointNet网络的变种（图2A），一个提取图像外观特征的CNN（图2B），以及一个融合网络，将两者结合起来并输出作物中物体的三维边界盒。我们描述了融合网络的两个变体：一个虚无的全局架构（图2C）和一个新颖的密集融合网络（图2D），其中我们使用密集的空间锚机制来改善3D框的预测，并使用两个评分函数来选择最佳预测。下面，我们将详细介绍我们的点云和融合子构件。

![image-20210626221534662](https://oj84-1259326782.cos.ap-chengdu.myqcloud.com/uPic/2021/06_26_image-20210626221534662.png)