分组建议、类别；相似性；实例分割

baseline：pointnet

深度图转点云，分割结果转检测结果

## 摘要

我们介绍了相似性分组建议网络（SGMN），这是一个简单而直观的深度学习框架，用于点云上的三维物体实例分割。SGPN使用一个单一的网络来预测**点分组建议和每个建议的相应语义类别，我们可以从中直接提取实例分割结果。**对于SGPN的有效性来说，重要的是**它以相似性矩阵的形式对三维实例分割结果进行了新颖的表示，它表明了嵌入式特征空间中每对点之间的相似性，从而为每个点产生了准确的分组建议。**在各种三维场景中的实验结果显示了我们的方法在三维实例分割上的有效性，我们还评估了SGPN改善三维物体检测和语义分割结果的能力。我们还通过将二维CNN特征无缝整合到框架中以提高性能来证明其灵活性。



## Experiments

1. Stanford 3D Indoor Semantics Dataset (S3DIS)

2. NYUV2[42]。部分点云是由单视图RGBD图像生成的。该数据集用三维边界框和二维语义分割掩码进行了注释。我们使用[11]中改进的注释。由于同时给出了三维边界框和二维分割掩码的注释，因此可以很容易地生成点云的三维实例分割标签。 我们采用795张训练图像和654张测试图像的标准分割。
3. ShapeNet

### S3DIS Instance Segmentation and 3D Object Detection

我们还展示了SGPN改善语义分割和三维物体检测的能力。为了验证SGPN的灵活性，我们还将二维CNN特征无缝地融入我们的网络，以提高NYUV2数据集的性能。

我们在斯坦福3D室内语义数据集上进行了实验，以评估我们在大型真实场景扫描上的表现。按照PointNet[33]的实验设置，点被均匀地采样到面积为1米×1米的块中。每个点都被标记为13个类别中的一个（椅子、桌子、地板、墙壁、杂物等），并由一个9D向量表示（XYZ、RGB和与房间有关的标准化位置）。

在训练时，我们对每个区块的<u>4096个点</u>进行均匀采样，在测试时，我们使用区块中的<u>所有点</u>作为输入。

SGPN在本实验中使用PointNet作为其基线结构。图5显示了S3DIS上用SGPN进行实例分割的结果。不同的颜色代表不同的实例。由于对象实例是无序的，所以同一组的点的颜色不一定与地面真相中的对应颜色相同。为了使实例类别可视化，我们还添加了语义分割的结果。SGPN在各种房间类型上都取得了良好的性能。

我们还用以下方法（我们称之为Seg-Cluster）来比较实例分割的性能。使用我们的网络进行语义分割，然后选择所有的点作为种子。从种子点开始，用BFS来搜索具有相同标签的相邻点。如果发现一个有200多个点的集群，它就被看作是一个有效的组。然后，我们的群组合并算法被用来合并这些有效的群组。

我们计算每个预测组和地面真实组之间的点的IoU。如果IoU得分大于阈值，则检测到的实例被认为是真阳性。平均精度（AP）被进一步计算用于实例分割的性能评估。表1显示了IoU阈值为0.5的每个类别的AP。据我们所知，目前还没有针对任意物体类别的点云实例分割方法，因此我们在表1中加入Armeni等人[1]对S3DIS的三维检测结果，进一步证明了SGPN处理各种物体的能力。我们的方法和[1]之间评价指标的差异是，[1]的IoU阈值是在三维边界框上的0.5，而我们的方法的IoU计算是在点上。尽管有这种指标上的差异，我们仍然可以看到我们在大型和小型物体上的卓越表现。我们看到，像Seg-Cluster这样的天真方法，对于像天花板和地板这样的大型物体，往往能正确地分离出远处的区域。然而，对于小物体，如果它们彼此靠近，Seg-Cluster就不能将具有相同标签的实例分开。表2中还评估了不同IoU阈值（0.25，0.5，0.75）的平均APs。

**一旦我们有了实例分割结果，我们就可以计算每个实例的边界盒，从而产生三维物体检测预测。在表3中，我们将我们的方法与PointNet[33]中介绍的三维物体检测系统进行比较，就我们所知，PointNet是S3DIS上最先进的三维检测方法。检测性能是在4个类别的AP上评估的，IoU阈值为0.5。**

PointNet中介绍的方法对给定的语义分割结果进行聚类，并对每个类别使用二元分类网络来分离具有相同类别的接近物体。

**我们的方法在很大程度上优于它，而且与PointNet不同，它不需要额外的网络，这在训练和测试时间内不必要地引入了额外的复杂性，在训练时间内引入了局部最小值。**SGPN可以有效地分离<u>同一语义类别但不同实例的对象</u>的困难情况（参见图4），因为**不同实例中的点在特征空间中相距甚远**，尽管它们具有相同的语义标签。我们在表4中进一步比较了我们与PointNet的语义分割结果。<u>SGPN在其相似性矩阵的帮助下胜过其基准线。</u>

<img src="https://oj84-1259326782.cos.ap-chengdu.myqcloud.com/uPic/2021/05_30_image-20210530175243053.png" alt="image-20210530175243053" style="zoom:50%;" />

### NYUV2 Object Detection and Instance Segmentation Evaluation

我们在NYUV2数据集上评估了我们的方法对部分三维扫描的有效性。**在这个数据集中，三维点云是从单一的RGBD图像中提取的。**一个大小为H×W的图像可以产生H×W的点。我们通过将图像的大小调整为H/4×W/4来对这个点云进行子采样，并使用近邻搜索得到相应的点。我们的训练和测试实验都是在这样一个点云上进行的。PointNet++被用作我们的基线。

【二维CNN结合3D点云】在[36]中，二维CNN特征结合三维点云进行RGBD语义分割。通过利用SGPN的灵活性，**我们也无缝整合了来自RGB图像的二维CNN特征以提高性能。**

一个二维CNN消耗一个RGBD map并提取大小为<u>H/4 × W/4 × NF2</u>的特征图F2。由于每幅图像都有H/4×W/4个子采样点，因此可以从**每个像素位置中提取一个大小为NF2的特征向量。**（来自于RGB：NF2）；

每个特征向量都与每个相应点的F（由PointNet/PointNet++产生的Np×NF特征矩阵）相连接；

**产生一个大小为NP×(NF+NF2）的特征图，然后我们将其输入到输出分支。**

<img src="https://oj84-1259326782.cos.ap-chengdu.myqcloud.com/uPic/2021/05_30_image-20210530182712478.png" alt="image-20210530182712478" style="zoom:50%;" />

我们称这一管道为SGPN-CNN。在我们的实验中，我们使用预先训练好的AlexNet模型[21]（第一层跨度为1）并从conv5层提取F2。我们使用H×W=316×415，Np=8137。二维CNN和SGPN被联合训练。

对19个物体类别进行了评估。图7显示了SGPN的实例分割的定性结果。表5显示了Seg-Cluster与我们的SGPN和CNN-SGPN框架在实例分割方面的比较。评价指标是平均精度（AP），IoU阈值为0.5。

（NYUV2 - Segment：与SegCluster相比，SGPN的改进幅度没有在S3DIS上那么高，因为在这个数据集中，具有相同语义标签的对象在欧氏空间中通常相距甚远。此外，像Seg-Cluster这样的天真方法也有好处，因为在部分扫描中，由于闭塞，点与点之间没有联系，所以很容易将单个实例分成若干部分。表5还说明，SGPN可以有效地利用CNN的特征。我们没有像[45]那样将二维和三维网络的全连接层连接起来，而是通过考虑其几何关系将二维和三维特征结合起来。）

我们进一步用实例分割的结果来计算边界盒。表6将我们的工作与NYUV2三维物体检测的最先进的工作[45, 11]进行了比较。按照[44]中的评估指标，在三维边界盒上用IoU阈值0.25计算AP。NYUV2数据集提供了地面真实的三维边界框，它囊括了整个物体，包括在深度图像中不可见的部分。[45]和[11]都使用这些大的地面真实边界盒进行推理。

**在我们的方法中，我们推断的是点的分组，它缺乏物体不可见部分的信息。**我们的输出是围绕部分扫描中的分组点推导出的紧密边界盒，这使得我们的IoU比[45, 11]小得多。**然而，我们仍然可以看到SGPN在局部扫描的三维物体检测任务上的有效性，因为我们的方法在小物体上实现了更好的性能。**

<img src="https://oj84-1259326782.cos.ap-chengdu.myqcloud.com/uPic/2021/05_30_image-20210530184809194.png" alt="image-20210530184809194" style="zoom:50%;" />

**计算速度** 

为了对测试时间与[45, 11]进行基准测试并进行公平的比较，我们在Nvidia K40 GPU上运行我们的框架。

SGPN每个样本需要170ms和大约400M的GPU内存。

CNN-SGPN需要300毫秒，每个样本需要1.4G的GPU内存。

GroupMerging在Intel i7 CPU上需要180毫秒。

然而，[11]中的检测网在Nvidia Titan X GPU上需要739ms。

在[45]中，RPN需要5.62秒，

ORN在Nvidia K40 GPU上每幅图像需要13.93秒。

我们的模型提高了效率，并在很大程度上减少了GPU的内存使用。