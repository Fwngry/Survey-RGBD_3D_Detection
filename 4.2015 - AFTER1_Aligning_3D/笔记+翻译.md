> 实例分割输出（HHA）基础上，使用的神经网络推断出物体的粗略姿势，解决对齐问题
>
> 吸纳CAD（模型搜索：初始化R、T - 模型配准：更新R、T - 模型选择）

## Abstract

这项工作的目标是用图书馆中相应的三维模型来表示RGB-D场景中的物体。**我们通过检测和分割场景中的物体实例来解决这个问题，然后使用卷积神经网络（CNN）来预测物体的姿势。**这个CNN是利用包含合成物体渲染图的图像中的像素表面法线进行训练的。在真实数据上测试时，我们的方法优于在真实数据上训练的其他算法。

然后，我们使用这个粗略的姿势估计和推断出的像素支持，将少量的原型模型与数据对齐，并将最适合的模型放入场景中。我们观察到三维检测任务的性能比目前最先进的技术[34]相对提高了48%，同时速度快了一个数量级。

## 1. Introduction

从机器人学的角度来看，这些任务中的每一项本身都不足以完成轨迹优化、运动规划或抓取估计等任务。我们提出的系统从一个<u>杂乱的室内场景的单一RGBD图像</u>开始，并产生图1所示的可视化输出。我们的方法能够成功地检索出相关的模型，并将它们与数据对齐。我们相信这样的输出表示将使感知在机器人等领域的应用成为可能。

图2描述了我们的方法。我们使用检测和分割系统的输出[13]，并首先使用神经网络推断每个被检测物体的姿势。我们使用表面法线图像而不是深度图像作为输入，在合成数据上训练这个CNN。我们表明，这个**在合成数据上训练的CNN比在真实数据上训练的CNN效果更好**。

（姿势假设作为初始化）然后，我们使用推断出的前k个姿势假设，来**初始化**对一小部分三维模型、它们的比例和确切位置的搜索。我们使用修改过的迭代最接近点（ICP）算法来完成这项任务。

（初始化的重要性）如果初始化得当，即使在物体类别而不是精确实例的层面上工作（ICP通常用于这种情况），它也能提供合理的结果。在这样做的时候，我们只使用图像上的二维注释来训练我们所有的模型，并且在测试时，能够生成丰富的三维场景表示。

<img src="https://oj84-1259326782.cos.ap-chengdu.myqcloud.com/uPic/2021/05_24_05_24_image-20210524172418066.png" alt="image-20210524172418066" style="zoom:50%;" />

我们的最终输出是一组三维模型，这些模型已经与图像中的物体对齐。当我们与目前最先进的3D检测方法进行比较时，我们系统输出的丰富性和质量就会显现出来。我们输出的一个自然副产品natural side-product 是场景中每个物体的三维边界盒。当我们使用这个三维边界框进行三维检测时，我们观察到比目前最先进的方法（"滑动形状"）[34]有19%的绝对AP点（48%相对）的改进，同时至少快一个数量级。

## 2.Related Work

最近，在[13]中，提出了一个深度图像的地心嵌入，即水平差异、离地高度和与重力的夹角，以便用CNN学习自下而上的边界盒建议的特征（HHA）。该方法还产生了一个实例分割，属于被检测物体的像素被标记。[18, 24]以类似的模式对自下而上的区域建议进行推理，但重点是对物体-物体、物体-场景和图像-文本关系进行建模。

在本文中，我们在具有挑战性的NYUD2数据集的背景下研究了这个问题，并分析了RGB-D数据如何能够有效地用于这个任务。

1. 与我们的工作最相关的研究来自Song and Xiao [34]和Guo and Hoiem [10]。Song和Xiao[34]在3D中进行推理，使用合成数据训练典范SVM，并在3D空间中滑动这些典范来搜索物体，从而自然地处理遮挡问题。他们的方法很有启发性，但计算成本很高（每张图片每个类别25分钟）。他们还展示了一些例子，其中他们的模型能够将一个好的典范放在数据上，但他们没有解决估计好的三维模型来适应数据的问题。**我们与他们的理念不同，建议对问题进行二维推理，以有效修剪搜索空间的大部分，然后对前几位获胜的候选人进行详细的三维推理。因此，我们的最终系统的速度要快得多（每张图片大约需要两分钟）。我们还表明，从二维表示法提升到三维表示法是可能的，并表明在检测到的区域周围天真地确定一个盒子，其性能优于[34]的模型。**

2. Guo和Hoeim[10]从一个自下而上的分割开始，从训练集中检索最近的neiborhood，并将检索到的候选者与数据对齐。
   1. 相比之下，我们以自上而下的物体检测器的形式使用类别知识，并将物体的方向告知搜索程序。
   2. 此外，我们的算法不依赖于[10]中使用的那种形式的详细注释（每个场景大约需要5分钟）[9]。
   3. 我们还提出了一个类别级别的指标来评估这种算法的丰富和详细的输出。

最后，[28, 31]等许多人研究了同样的问题，但要么考虑已知的物体实例，要么依赖用户互动。

## 3.估算粗略的姿态（略）

在这一节中，我们提出了一个卷积神经网络来估计深度图像中刚性物体的粗略姿态。当代的工作[38]在RGB图像上研究这个问题。

假设C(k, n, s)是一个卷积层，核大小为k×k，n个单元，跨度为s，P {max,ave}是一个最大或最小值。(k, s)是内核大小为k×k、步长为s的最大或平均集合层，N是局部响应归一化层，RL是整流线性单元，D(r)是辍学层，dropout为r。我们的网络有如下结构。

作为网络的输入，我们使用3个通道的表面法线图像，其中三个通道使用法线矢量与用[12]中的重力估计算法得到的三个地心方向的角度来编码Nx、Ny和Nz。

1. 考虑到为这样一个详细的任务获得可靠的注释是非常具有挑战性的[9]，我们使用ModelNet[40]中的三维模型来训练网络。特别是，我们使用模型的子集作为训练集的一部分，并使用模型与典型姿势一致的10个类别（浴缸、床、椅子、桌子、梳妆台、显示器、床头柜、沙发、桌子、厕所）我们为每个类别抽取50个模型，并为每个模型渲染10个不同的姿势，这些姿势放置在一个水平的地板上，其位置和比例由NYUD2数据集[32]估计（一些例子在补充材料中提供）。我们在每个场景中放置一个物体，并取样与地面真实盒重叠超过70%的盒子作为训练例子。
2. 我们以与Girshick等人[8]相同的方式裁剪和翘曲边界盒。注意，翘曲法线可以保留所代表的角度（与翘曲深度图像或HHA图像[13]不同，后者会改变所代表的表面的方向）。

我们使用softmax回归损失来训练这个网络的分类，并在不同的类别中共享网络的下层。

我们还采用了地心约束，假设物体停留在一个表面上，因此必须放置在地面上。因此，我们只需要确定物体在地心坐标框架中的方位角。

在测试时，我们只需通过网络向前传播图像，并将输出的姿势仓作为预测的姿势估计。鉴于下一阶段需要一个良好的初始化，在实验部分，我们对预测的前k(=2)个模式进行研究。

## 4.模型对齐

我们现在考虑在场景中放置一个三维物体模型的问题。**我们从[13]的实例分割输出（HHA）开始，使用的神经网络推断出物体的粗略姿势。**

有了这个对物体的像素支持的粗略估计和对其姿势的粗略估计，我们解决一个对齐问题，以获得物体在场景中的最佳位置。

### 4.1. 模型搜索

请注意，我们的姿势估计器只为模型提供了一个方向。它没有告知物体的大小，也没有告知哪个模型最适合该物体。<u>因此，在这一阶段，算法在规模和CAD模型之间进行搜索，为每个候选模型推断出一个最佳的旋转R和翻译t。the algorithm searches over scales and CAD models, inferring an optimal rotation R and translation t for each candidate.</u>

为了搜索 **规模**，我们从[9]的3D BBOX注释中收集类别级别的统计数据。特别是，我们使用顶视图 in the top view 中的边界框的面积，估计该面积的平均值及其标准差，并从N(µ area , σ area)中获取N个规模分层样本。这样的统计数据不需要注释，也可以从在线家具目录中获得。为了搜索规模，我们对每个模型**进行各向同性的缩放**，使其在顶视图中具有采样的面积。

为了对 **模型** 进行搜索，我们为每个类别的3D模型**选择少量的N个模型**（在我们的实验中为5个）。注意挑选不同的模型，但这种选择也可以以数据驱动的方式进行（通过挑选能很好解释数据的模型）。

最后，我们**使用迭代最接近点（ICP）[27]对R和t进行迭代优化**，我们通过约束旋转估计与重力方向一致来修改。

初始化：

1. 我们使用从第3节得到的**姿势估计和推断的重力方向**[12]来初始化R。
2. 我们使用分割遮罩中各点的世界坐标的**中位数来初始化平移分量**tx和tz，并设置ty，使模型靠在地板上（这种约束有助于处理严重遮挡的物体，例如椅子，通常只有背部可见）。

### 4.2. 模型配准

1. 模型对齐算法

模型对齐算法的**输入**是 <u>深度图像D、分割遮罩S、给定比例s的三维模型M和模型的初始估计变换（旋转矩阵R0和翻译矢量t0）</u>。该算法的**输出**是<u>一个旋转R和一个变换t</u>，这样，用变换R和t渲染的三维模型M就能解释分割遮罩S中尽可能多的点。

2. Render model 渲染模型

使用当前估计的变换参数（s, R, t）来渲染模型M，获得模型的深度图像。从深度图像中投射属于分割遮罩S的点（得到点集P对象，以及从渲染的模型深度图像中投射到三维空间的点（得到点集P模型）。

3. 重新估计模型转换参数

运行ICP，将P对象中的点与P模型中的点对齐。

我们通过将P对象中的每个点与P模型中最接近的点联系起来形成对应关系，这可以防止对象中的遮挡点的联系。

我们还根据距离拒绝最差的20%的匹配。

最后，在估计变换（R，t）的更新时，我们对旋转矩阵R实施额外的约束，使其只围绕重力方向运行。

### 4.3. 模型选择

现在我们需要在N个规模的N个候选模型中选择最能解释数据的模型。我们将这种**选择作为一个学习问题，并计算一组特征来捕捉对数据的解释的质量。**我们计算了以下特征：

（渲染-遮挡）渲染模型中被遮挡的像素的数量和比值fraction，这些像素是由数据解释的；（分割-被解释）输入实例分割中被模型解释的像素的比值fraction和数量；（分割-掩码交集）实例分割与数据解释的模型掩码的交集；（掩码-未被遮挡）以及未被遮挡的模型的掩码。

我们在这些特征上学习一个线性分类器，以挑选出最佳模型。这个分类器是用来自渲染模型的正数来训练的，这些模型与地面真实区域有50%以上的重叠。

## 5.实验

我们在Silberman等人[32]的NYUD2数据集上评估我们的方法，并使用795张图片的标准训练集和654张图片的测试集。我们将795张训练图像分成381张训练图像和414张验证图像。对于合成数据，我们使用Wu等人[40]提供的对齐模型集合。

### 5.2. Model Fitting

我们首先描述和评估我们用来估计目标对象像素支持度的模型拟合程序的实例分割系统。

然后，我们说明如何准确地将我们的二维输出提升到三维。我们与[34]和[10]的三维检测任务进行比较。

接下来，我们提出了一个新的指标来评估三维模型的放置，并提出了模型排列算法中设计选择的控制实验。

最后，我们展示了我们输出的例子。

#### 5.2.1 对象检测和实例分割

我们注意到，我们的实例分割系统[13]在边界框上计算CNN特征，而不是自由形式的区域。我们实验了<u>除了盒子上的特征外，还在被遮挡的区域上计算了特征</u>[14]，并观察到这些额外的信息提高了边界盒检测以及实例分割的性能，从而在这些任务上取得了最先进的结果（表1）。

APb从35.9%上升到39.3%，APr从32.1%[13]提高到34.0%。在[13]中，模型只对381张训练图像进行了ﬁnetuning，当对795张训练图像进行ﬁnetuning时，APb和APr都有进一步提高（表1中第4行和第7行）。

我们在这项工作中使用这些最终的实例分割。当然，人们可以进一步调整这些区域[13，14]，以获得更好的实例分割，但我们选择了 但我们选择使用这个中间输出，以减少我们在相同数据上的训练次数。

模型对齐的性能指标 鉴于我们算法的输出是一个放置在场景中的三维模型，如何评估性能并不明显。人们可能会想到评估个别任务，如姿势估计、子类型分类、关键点预测或实例分割，但独立完成这些任务并不能衡量三维模型放置的性能。此外，对于我们所考虑的许多类别，可能没有一个一致的姿势定义（如桌子），或关键点（如沙发），或子类型（如椅子）的定义。

因此，为了衡量在场景中放置3D模型的性能，我们提出了一个新的指标，直接评估推断出的模型与观察到的深度图像的吻合程度。我们假设有一个固定的三维模型库L，而一个给定的算法A必须从这些模型中挑选一个，并将其适当地放在场景中。我们假设我们有类别级别的实例分割注释。