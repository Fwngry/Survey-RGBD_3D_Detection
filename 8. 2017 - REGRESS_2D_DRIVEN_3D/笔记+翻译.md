> Open source： https://github.com/phoenixnn/ 
>
> NYUV2数据集
>
> 输入RGBD；2.5D表示；输出3D框位置、尺寸、方向
>
> 思路：首先，2D初始化类别的位置大小；之后，3D位置回归

本文讨论了三维物体检测的模态感知问题。其任务是不仅要在三维世界中找到物体的定位，还要估计它们的物理尺寸和姿势，即使在<u>RGB-D图像</u>中只有部分物体是可见的。

最近的方法试图利用深度通道中的点云来直接利用三维空间中的三维特征，并证明了其比传统的2.5D表示方法的优越性。我们通过坚持<u>2.5D表示法</u>框架，重新审视了正负三维检测问题，并将2.5D视觉外观与三维物体直接联系起来。（深度图像的点云来利用三维特征）

我们提出了一个新颖的三维物体检测系统，它可以同时预测室内场景中物体的三维位置、物理尺寸和方向。

在NYUV2数据集上的实验表明，我们的算法明显优于最先进的算法，并表明2.5D表示法能够为三维正交物体检测编码特征。所有源代码和数据都在

https://github.com/phoenixnn/Amodal3Det

## 简介

（背景）尽管二维矩形可以大致表明物体在图像平面上的位置，但由于多种因素，如遮挡和透视投影产生的不确定性，它们在物理三维世界中的真实位置和姿势<u>很难确定</u>。然而，对人类来说，从静止的图像中了解物体离观看者有多远、物体的姿势和它们的全部范围是<u>非常自然</u>的。这些特征对于许多应用来说是非常理想的（如机器人导航、抓取估计和增强现实（AR）等。为了填补这一空白，在过去的十年中做出了各种努力，包括从单眼图像中推断出三维物体定位）

（中间的工作）为了填补这一空白，在过去十年中做出了各种努力，包括从单眼图像中推断三维物体定位[6, 13, 20, 3]，以及CAD模型上的三维物体识别[29, 27]。但是这些工作要么是通过假设位置是已知的而依赖于大量的理想三维图形模型，要么是在杂乱的环境中倾向于失败，因为在这种环境中，遮挡是非常普遍的，而深度顺序是不确定的。

（RGB-D方向）最近微软Kinect和类似传感器的出现缓解了其中的一些挑战，从而使3D物体检测的方法有了一个令人兴奋的新方向[18, 17, 12, 11, 19, 25, 24]。Kinect配备了一个主动红外结构光传感器，能够提供与物体视觉外观相关的更准确的深度位置。根据从RGB-深度图像中制定特征表示的方式，RGB-深度检测方法可以大致分为两组。

（2.5D方法）一般来说，2.5D方法首先利用（彩色和深度图像的适当特征2.5D表示）进行物体（检测），并建立模型将2D结果转换为3D空间。

（3D方法）而3D方法则是直接将检测建议放入三维空间，在三维窗口内从三维点云中提取特征（转为3D特征，检测）。确定2.5D或3D方法是否代表3D正交物体检测的正确方向的竞争是超级激烈的。

（截至目前）[25]利用三维滑动窗口直接推断出三维空间中的检测结果，并证明其在处理遮挡、视点等方面比2.5D方法有优势。然后，2.5D方法[11]通过从成熟的二维推理开始，将CAD模型与二维检测对齐，表现优于[25]。最近的工作[24]通过引入3D ConvNet模型来直接编码3D几何特征，比[11]的表现要好得多。到目前为止，在具有挑战性的NYUV2 RGB-Depth数据集[22]上，以3D为中心的滑动形状的3D检测性能领先。

（3D方法的困难）尽管利用三维几何特征进行检测是有希望的，实践中，重建的三维形状往往是不完整的（当把一个单一的深度图的像素投射到三维空间时）、嘈杂和稀疏的（由于闭塞、反射和红外光的吸收）。因此，获得的表面质量与CAD模型的360度全景图有很大的不同，这使得将三维边界盒固定在三维点上成为一项非常具有挑战性的任务。特别是，当深度图上的大部分物体区域处于 "黑洞 "中时，恢复的三维形状很难提供突出的特征。

（2.5D方法）记录在二维图像平面上的光信号是密集的、有结构的，人类仍然可以从这些图像中感知物体并估计它们的三维位置。因此，使用目前的深度学习技术应该可以模仿人类的3D感知并直接利用2.5D图像特征。正如所提出的方法所证明的那样，情况确实如此。

<u>在本文中，我们从二维的角度重新审视三维正交物体检测问题。</u>我们从从扩展的多尺度组合分组（MCG）类独立物体建议中获得的二维边界盒建议开始[1，12]。我们设计了一个基于Fast-RCNN框架的新型3D检测神经网络，该网络自然地将深度信息与相应的视觉外观结合起来，以同时识别室内场景中的物体类别、方向和它们的全部范围，其中超级像素周围的2D边界框与RGB-深度图像一起被作为输入。总而言之，这项工作的主要贡献有以下几点。

- 据我们所知，我们是第一个将三维模态检测问题重新表述为仅基于2.5D图像外观特征的类三维边界框模型的回归。
- 考虑到<u>颜色、深度图像和二维分割建议</u>，我们设计了一个新颖的三维检测<u>神经网络</u>，可以同时预测三维物体的位置、尺寸和方向。不需要在深度特征上训练SVM或将三维CAD模型纳入二维检测的额外步骤。
- 我们没有像<u>三维检测器[25, 24]那样做任何曼哈顿世界的假设来估计方向</u>，因为房间里的物体往往是杂乱无章的，反映了各种生活方式，这样的假设可能对移动机器人等自主系统产生危险的后果。
- 此外，为了有利于未来的模态三维检测研究，我们<u>改进了NYUV2数据集</u>的三维真实边界盒，消除了许多错误，如错误的标签、部分范围、假阴性等。

## 相关工作

来推断出图像平面内可见物体部分的边界盒。由于人类可以毫不费力地推断出物体的整体性和完整性，[16]通过解决二维图像平面上的全范围物体推断问题，向获得类似水平的感知能力又前进了一步。尽管这种物体表征比传统的模态推理更丰富，但它离人类在物理世界中的感知水平和一些机器人应用的要求还很远，在这些应用中，机器人被期望与环境互动。为了填补这一空白，越来越多的3D物体检测相关研究被提出来，特别是在消费市场上出现了主动式传感器之后。在下文中，我们将简要回顾一下RGB-D图像的3D检测算法。

**RGB-D图像中的2.5D方法。2.5D方法通常是指在传统的2D检测任务中，以类似于彩色图像的方式处理深度图像的方法。**17]通过利用深度图的三维欧几里得距离，将DPM算法适应于RGB-D图像。在现有的二维检测器的输出边界框内，从彩色图像中提取手工制作的特征，并在其相关的前景物体分割掩码内投影三维点云。他们的物体位置使用三维椭圆体进行参数化。[18]首先通过适应CPMC算法生成了三维候选立方体，然后将二维外观特征、物体-物体和物体-场景关系纳入条件随机场（CRF）模型进行语义标签推理。

最近，在基于二维图像的物体检测中，特征工程已经逐渐被深度卷积神经网络（CNN）所取代。最受欢迎的代表作品是R-CNN[10]、Fast-RCNN[8]和Faster-RCNN[9]。受[25]的启发，[19]在三维点云中采用了详尽的三维滑动窗口策略，通过向预训练的R-CNN和下面单独训练的双模深度Boltzman机器提供投影的2D边界框来提取跨模态特征。然后，检测结果由训练有素的典范SVM组合决定。RCNN_Depth<u>与之前的滑动窗口框架不同，[12]将他们的检测器建立在预先计算的物体分割候选者之上。他们扩展了R-CNN[10]，利用HHA编码（水平差异、离地高度和当地表面法线与重力方向的角度）的深度信息。然而，他们系统的输出仍然局限于二维边界盒。</u>11]对[12]进行了扩展，首先对每个检测到的二维物体进行三维粗略姿势估计，然后用迭代最接近点（ICP）算法将三维CAD模型与从深度图像投射回来的属于分割掩码的三维点对齐。

所提出的方法与之前的工作有三方面的区别。

1）没有利用额外的训练实例或三维CAD模型。

2）模型是端到端的训练，而不是零散的。

3）不需要从深度图中提取点云，而深度图往往因遮挡而产生噪声和不完整。

RGB-D图像中的3D方法：三维方法以一种不同的方式利用深度图，即首先重建三维点，主要的处理是基于分析点云。[25]将传统的二维滑动窗口策略扩展到三维，将三维盒子放在一个估计的房间盒子里。用三维CAD模型渲染的合成深度训练了一堆示范性SVM，然后应用于三维室内场景中的每个三维检测窗口。三维手工制作的特征是直接建立在离散的三维空间上的。尽管该方法显示了令人鼓舞的检测性能，但所需的计算非常昂贵。SS/DSS<u>[24]通过提出三维区域候选者并直接从三维卷积神经网络中提取三维特征，在性能和效率方面都大大改进了[25]。</u>与[25]类似，[21]在点云上设计了手工制作的三维特征，用于三维立方体检测和曼哈顿房间布局预测。为了更好地分析三维特征，[24]和[21]都利用了通过融合附近的多个深度图帧得出的增强型深度图来去噪和填补缺失的深度。

![image-20210525201904748](https://oj84-1259326782.cos.ap-chengdu.myqcloud.com/uPic/2021/05_25_05_25_image-20210525201904748.png)

图1. 对于每个二维段的提议，我们首先根据<u>深度信息初始化一个三维框</u>（黄色虚线框）的定位，并根据<u>类的先验知识确定其大小</u>。(深度信息-初始化三维框定位；类别先验-大小)

然后，仅根据二维特征联合学习物体<u>类别</u>和<u>三维回归偏移</u>，目的是通过调整初始三维方框的位置、尺寸和方向，获得最终的三维检测。（三维回归偏移、类别 - 尺寸、方向、位置）

## 3. 3D Object Detection in RGB-D Images

### 3.1. Amodal 3D Object Detection

给定一对彩色和深度图像，正交三维物体检测的目标是确定物体的

实例的位置和它在三维空间的全部范围。众所周知，现实生活中的典型室内环境是非常复杂的，因为物体可能被严重遮挡，并以广泛的构型出现。在三维CAD模型检索成功的鼓励下，可用的深度图使得直接对三维几何特征进行编码检测非常有希望。然而，由于测量误差，深度图的质量在现实中远非完美，更重要的是，物体实例的几何形状是不完整的，其变化是由相机视图决定的，例如，见图4中的例子。这可能会严重限制直接三维重建的表现力。因此，在本节中，我们**重新审视**RGB-D amodal物体检测的任务，并通过假设**2.5D特征表示与3D物体位置和方向之间存在潜在的关系来坚持2.5D表示**。在下文中，我们将探讨如何有效利用RGB和深度来完成这项任务。

1. 二维RoI建议：色彩和深度图像中包含的信息被各种RGB-D研究工作证明是**相互补充**的。颜色编码了独特的视觉外观特征，而深度则传达了物体的几何结构。然而，在三维检测中，一个额外的维度会**极大地扩大搜索空间**。<u>由于从成熟的二维推理开始，可以说比从三维推理开始更加有效和准确[11]。我们通过在RGB-D图像中使用适应的MCG算法获得ROI建议[12]。</u>

2. 3D box proposal and regression

   如图2所示，对于每个二维proposal，我们计算出一个三维框的对应物作为三维 proposal。然后，根据所学到的高水平2.5D表征，将3D proposal 转化为3D gt。

   <img src="https://oj84-1259326782.cos.ap-chengdu.myqcloud.com/uPic/2021/05_25_05_25_image-20210525214238000.png" alt="image-20210525214238000" style="zoom:33%;" />

   过程如图：虚线盒子 - （类别信息）- 实线盒子 - （回归）- 绿色盒子

   三维箱体建议是由相应的二维段体建议衍生出来的。对于三维方案中的盒子尺寸，我们简单地使用“从训练集中估计出的平均等级的盒子尺寸”作为“基础三维盒子尺寸”。这比从二维区段像素投射回来的三维点要好，因为这将大大增加回归时的盒子尺寸的方差。

   它的灵感来自于人类3D感知中熟悉的尺寸线索[7, 16]。例如，当人们在寻找像床这样的物体时，他们脑海中会有一个大致的物体尺寸，这就限制了对床实例的检测。

   3D点初始化：3D盒子[Xini, Yini, Zini]的中心是基于2D段像素投影的3D点初始化的。

   1. 由于深度图通常是有噪声的，并且有缺失的数据，为了稳健起见，我们将Zini设置为z中值，即分段点的中位深度值。
   2. Xini和Yini的计算方法如公式（3）所述：f是RGB相机的焦距，（Ox , Oy ）是主点，（Cx , Cy ）是二维方框建议的中心。

   <img src="https://oj84-1259326782.cos.ap-chengdu.myqcloud.com/uPic/2021/05_26_image-20210526101010284.png" alt="image-20210526101010284" style="zoom:50%;" />

   因为物体可能以不同的方向出现。方向角θ被明确地引入为三维边界盒模型的一个参数。我们将三维盒子的方向向量定义为垂直于其在XZ平面上的长边的向量（图2中的黄色向量）。对于所有的三维盒子建议，初始方向角θini被设置为零，即平行于倾斜坐标系中的x轴，这是在盒子方向矢量与相机主轴对齐的情况。θ的范围是[-π/2, π/2]

   （回归参数-代替-匹配差异）三维箱体回归器net，根据学习到的2.5维外观特征，重塑原始三维形状模型。在训练阶段，我们将每个阳性例子和地面实况盒子的回归偏移量表示为一个7元素的向量[δ x , δ y , δ z , δ l , δ w , δ h , δ θ]。我们可以直接比较相应的长、宽、高参数，并根据检测到的盒子的大小对其进行归一化处理；而不是找出检测到的盒子和gtbox之间主要方向的最接近匹配[24]来计算盒子的尺寸差异，这是由于我们对三维边界盒子的参数化。与[8]类似，学习的目标随后被来自提议的盒子的统计信息规范化。

3. Multi-task Loss

   每个训练实例都与gt类c和相应的gt3DBBox有关。为了联合训练分类和边界盒回归，损失函数被定义为：

   <img src="https://oj84-1259326782.cos.ap-chengdu.myqcloud.com/uPic/2021/05_26_image-20210526110619195.png" alt="image-20210526110619195" style="zoom:33%;" />

   其中，tc表示 regression与gt 3d位置的关系，v3d是回归目标，L3d是L1平滑损失；gt类c，p是预测的物体类别的概率，Lcls被定义为softmax函数。

4. Post processing

   我们对二维检测到的盒子采用典型的非最大抑制（NMS）方案。在三维中不使用NMS。与[24]相反，我们不对结果进行任何进一步的修剪，例如，基于物体大小的统计。

### 3.2. 卷积网络结构

最近有许多深度卷积网络模型被提出用于基于二维图像的识别。在本文中，我们采用Fast-RCNN[8]作为原始的基础模型，因为它的单阶段训练结构和通过共享特征计算的高能效性。如图1所示，彩色和深度图像分别经过两个VGG16[23]Conv-Nets计算共享特征图。基于二维物体建议的RoI池层所提取的特征及其放大的上下文斑块被串联起来用于多任务学习。
**小批量采样**

<img src="https://oj84-1259326782.cos.ap-chengdu.myqcloud.com/uPic/2021/05_25_image-20210525204635416.png" alt="image-20210525204635416" style="zoom: 33%;" />

在训练深度神经网络模型时，为了提高计算效率，从训练集中随机选择一小群例子，在每次迭代中更新模型参数。正确定义和选择基于图像的物体检测的RoI池中的正负例子是非常重要的。通常情况下，如果一个RoI与地面真实箱的交集大于联合（IoU），则被视为阳性，如果IoU在0.1和0.5之间，则被视为阴性。<u>然而，直接将这一规则应用于使用[26]提供的二维注释的小批量取样会造成一个严重的问题。[26]为NYUV2数据集提供了两种二维地面真实边界盒。1）通过对可见的点云进行精确定位而投射的二维边界框，以及2)从正交三维边界框投射的二维边界框。</u>如图3所示，使用这两种方法进行小批量取样，检测性能会大幅下降，因为如果直接与[26]提供的二维地面真相进行比较，真正的阳性片段可能会被视为阴性片段。

为了解决这个问题，我们在训练集中增加了名为gt2d sel的<u>新的二维地面实况框，用于确定仅由提议的二维片段构成的正负例子。</u>

1. 我们强调，由[26]提供的正交二维边界盒仍可作为二维盒回归任务的目标。每个小批次包括从N=2幅图像中随机选择的256个二维框建议（每幅图像128个RoI）。
2. 阳性和阴性例子的比例被设定为1 : 3。
3. 为了增加数据，我们在水平方向上剪辑图像和它们相应的三维边界盒。在训练过程中没有使用其他额外的数据。

在表1中，我们与最先进的三维方法算法 "深度滑动形状"[24]在NYUV2 RGB-D数据集的19类检测任务中进行了定量比较。**我们的方法明显优于[24]，按平均精度得分（mAP）衡量，明显高出4.6%。**

特别是，我们对[24]中报告的困难物体类别（如门、电视、盒子、显示器）实现了更好的检测性能。原因是在[24]中，3D盒子建议网络（RPN）依赖于恢复的3D点云的质量。但是，在实践中，来自Kinect alike传感器的深度数据是有噪声和不完整的。因此，如果点云是稀疏的或空的对象实例，如电视或显示器，那么相应的三维锚箱被视为负面的三维建议并被丢弃。相比之下，我们的方法在这种情况下更加稳健，因为我们的3D盒子初始化使用了分段像素深度的中值，而3D回归是基于学习到的2.5D特征（见第3.1节），因此既不依赖于3D点云的密度，也不依赖于3D点云的几何形状。

在测试过程中，检测网对每张RGB-D图像需要0.739秒，这比DSS中的物体识别网络（ORN）快了近20倍。

## 总结

我们提出了一个新颖的正交三维物体检测系统，该系统在RGB-D图像中直接学习深度特征，而不进行任何三维点重建。

我们的系统从彩色和深度图像对中学习2.5D视觉外观特征。

<u>实验证明，2.5D视觉特征与3D物体的大小、位置和方向相关。</u>

我们的方法明显优于性能最好的3D检测器[24]，这确实是一种3D方法，因为它分析的是3D点云。

