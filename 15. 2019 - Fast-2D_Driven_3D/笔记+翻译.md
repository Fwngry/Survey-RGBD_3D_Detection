2018年发布在二区上的2d驱动3d系列续作

## 摘要

RGB-D图像中的三维物体检测是计算机视觉中一个不断发展的巨大研究领域。在本文中，我们研究了RGB-D图像中的模态三维物体检测问题，并提出了一个能够预测物体位置、大小和方向的古老的三维物体检测系统。与现有的使用多阶段点云处理或预先计算的分割掩码来生成三维边界框的方法不同，我们只利用二维区域建议来完成这项任务。给定一对彩色和深度图像作为输入，我们首先从设计的多模态融合区域建议网络中预测二维区域建议，然后我们提出了一种efficient 方法，通过将二维边界框用一个比例系数缩小并投射到三维空间，从这些区域建议中生成三维边界框。我们在具有挑战性的NYUv2和SUN RGB-D数据集上评估了我们的系统，并与最先进的检测方法进行比较。实验结果表明，我们的方法以较快的检测时间明显优于最先进的方法。我们在NYUv2数据集的19类物体检测任务中取得了最好的结果，而在SUN RGB-D数据集的10类物体检测任务中，我们的检测性能也相当快。

## 简介

物体检测是计算机视觉的基本挑战之一。在过去的几年里，二维物体检测得到了广泛的研究，由于大规模注释数据集的可用性和深度卷积神经网络（CNN）的令人印象深刻的进展，目前显示出巨大的性能[12, 11, 36]。二维物体检测的目的是通过在图像平面上绘制矩形来识别和定位所有物体。然而，超越二维平面信息，在三维世界的室内场景中，准确的三维物体检测与之相比仍然是一个开放的问题。三维理解在众多现实世界的应用中非常重要，如自主导航、管家机器人、自主驾驶和增强现实（AR）。为了促进室内场景的发展，基本问题是能够可靠地分类和定位三维空间中的模数三维物体，其中模数三维物体检测旨在绘制一个完整物体的三维边界框，即使该物体的一部分被遮挡或截断。三维物体检测使机器（即机器人）能够与室内三维环境互动。在过去的十年中，人们做了各种尝试，从单眼图像[29, 10]或插入CAD模型[13, 45, 47]中定位三维物体；或将二维检测扩展到三维[27, 18]，但这些技术需要对环境的预先了解。
最近，随着技术的快速发展和低成本3D传感设备（即微软Kinect、Xtion Pro-live等）的普及，在提供深度和颜色信息的同时，RGB-D图像中的室内场景理解成为一个活跃的研究领域[6, 48, 46]。深度图像提供了更多的几何信息，与RGB图像相比，它对颜色、光照、旋转和比例都是不变的。因此，利用深层神经网络的RGB-D数据的力量可以极大地提高图像/视频分类任务的性能[39, 16, 33]。然而，由于缺乏像ImageNet[7]那样的大规模RGB-D注释数据集，以及室内场景中的大量遮挡物，3D物体检测本身是一项具有挑战性的任务，对室内场景来说难度很大。此外，3D的主要缺点是增加了一个额外的空间维度，使得物体的搜索空间明显变大，即使在现代强大的硬件加速器（GPU）上运行，也需要大量计算。因此，最先进的3D物体检测方法仍然倾向于比2D物体检测方法慢得多。

文献中主流的物体检测框架包含两个重要的组成部分：建议生成阶段[36, 44]和区域性物体识别阶段[12, 20]。

**（区域提议网络 - 3D计算量大）**与传统的基于滑动窗口的方法[9]不同，最近提议生成方法的发展旨在提出适量的候选区域，这些区域涵盖了大部分的地面真实物体，并得到了深度学习技术（即区域提议网络[36]）的好处。然而，从三维点云处理[44, 26, 37]或三维卷积神经网络[44, 23]开始，三维物体检测的计算量很大。

**（点云 to 图像/体素网格；直接使用点云）**大多数现有的三维物体检测工作都是从三维点云开始的，将其转换为图像[45, 31]或体积网格[47, 31, 28]，或直接使用它们[32, 30]作为卷积神经网络的输入来训练。例如，[44]将整个场景的点云转换为三维体素网格，并使用三维卷积神经网络进行物体提议和分类。

**（困难：嘈杂消失、区域为空）**尽管这些工作在一些三维理解任务中取得了eﬃciency，但从室内深度传感器获得的深度数据中创建的点云往往是嘈杂、稀疏和不完整的。**因此，如果一个物体由于这种嘈杂的深度数据而从点云中消失，或者它在深度图上的大部分区域是空的，那么三维锚定盒就不能识别该物体，从而导致性能下降。**

此外，这些方法将整个场景的点云转换为另一种形式，并在3D ConvNet上搜索物体的候选框，其计算成本通常相当高。最近的方法[21, 8]试图从二维角度解决三维物体检测问题。

2D-Drievn-3D-Frustum[21]需要手工制作的特征，这些特征被送入多层感知器（MLP）网络来回归三维盒子的位置和姿势。

2D-Driven-3D-Regress[8]将超级像素周围的二维边界盒与RGB-深度数据一起作为输入。

**但是，他们的方法需要为每一个由多尺度组合分组（MCG）算法[14]产生的二维建议提供分割掩码信息，这需要额外的和精确的计算。**

## 相关工作

长期以来，物体检测一直是计算机视觉中的一个挑战性问题。随着深度卷积神经网络的迅速成功，最近几年见证了物体检测技术的革命性进展。最先进的如R-CNN[12]、Fast R-CNN[11]、Faster R-CNN[36]、YOLO[34, 35]、SSD[25]、Mask-RCNN[17]和Focal Loss[24]是在检测图像中物体的速度和准确性方面最为成功的方法。然而，这些方法都是为预测给定图像中物体可见部分周围的二维矩形边界框而设计的。**然而，就速度和准确性而言，在三维物体检测方面很少有成功的工作。**下面，我们简要地回顾一下RGB-D图像中现有的物体检测算法。基于二维和三维，我们可以将其分为两部分。

**2.1. RGB-D图像中的2D物体检测**

早期，RGB-D物体识别和检测工作依赖于通道规格的手工设计的特征描述符，将深度作为一个额外的通道。

在[22]中，Lai等人首先介绍了RGB-D物体数据集，并提出了一个**物体识别和检测框架**，其中<u>物体识别</u>是通过使用几个手工制作的**特征**（即SIFT描述符、文本直方图和颜色直方图）与自旋图像和形状特征表示相结合来进行的。而<u>物体检测</u>是通过计算RGB和深度图像上的**定向梯度直方图（HOG）**的变体和归一化深度直方图来进行的。另一方面，<u>基于学习的特征描述符learning-based feature descriptors</u> 为特征提取技术开辟了新的视角。

Blum等人[2]从RGB-D数据中探索了一种基于特征学习的方法，其中卷积K-means描述器可以从检测到的具有SURF特征的兴趣点附近自动学习特征。

Bo等人[3]提出了一种基于稀疏编码的层次匹配追求（HMP）方法，以无监督的方式从RGB-D图像中学习新的特征表示。**深度卷积神经网络（CNN）在RGB图像上进行二维物体检测的成功，使得研究人员最近开始探索在RGB-D图像上使用CNN。**

✅Gupta等人[14]首次提出了一个基于深度R-CNN的物体检测器，在RGB-D图像上使用深度CNN。与使用彩色图像作为输入的R-CNN不同，他们将深度作为额外的输入流，将其编码为三个通道，作为离地高度、水平差异和重力角（HHA）的组合。尽管在大规模图像数据集（即ImageNet[7]）上学习的预训练模型的初始化大大加快了RGB图像上的物体检测，但是，对于深度模式，没有这样的大规模数据集，可以作为预训练模型使用。Gupta等人在[14]中使用ImageNet预训练模型来初始化颜色和深度参数，后来他们在[15]中提出了一个跨模型监督转移技术，他们<u>利用从一个有大量标签的模态中学习的表征作为监督信号</u>来训练无标签的配对深度模态的表征。

**2.2. RGB-D图像中的3D物体检测** 

✅最近，旨在检测RGB-D图像中的3D物体的方法利用了具有体素网格表示的3D点云[SS]。例如，[43]通过将一组三维CAD模型渲染成合成深度图，在三维点云上计算出的手工制作的特征上训练模范SVM分类器。在测试过程中，他们在三维空间中滑动一个三维检测窗口，以匹配典范形状和每个窗口。此外，他们还使用深度分割来提高性能。

✅Gupta等人在[13]中扩展了深度R-CNN[14]，用于生成三维边界框，方法是将三维CAD模型对准从二维识别结果中投射回来的三维点，使用修改过的迭代最接近点（ICP）算法的分割面具。

受[43]的启发，Liu等人[26]也在三维点云中采用了三维滑动窗口策略，以获得所有典范-SVMs的分数。三维边界框被投射到两个通道的二维边界框中，然后通过将其分别送入预训练的R-CNN和双模深度玻尔兹曼机来提取跨模态特征。最后，模范-SVMs被用于检测。然而，许多示范分类器和手工制作的特征的使用使得该算法的计算量很大。

✅与[43]类似，Ren等人在[37]中设计了一个定向梯度云（COG）描述符用于点云中的三维物体检测，他们在[38]中通过使用潜在支持面进一步扩展了他们的工作。尽管Ren等人在mAP上获得了显著的收益，但他们的模型在测试时对每幅图像需要10-30分钟的处理时间。

✅[43]在Deep Sliding Shapes[44]中进行了扩展，其中Song和Xiao受到更快的R-CNN[36]的启发，提出了一种基于3D ConvNet的方法，将整个场景的点云转换为3D体积网格并输出3D边界框。他们的方法有两个模块：一个三维区域建议网络（3D RPN）用于生成<u>三维区域建议</u>；一个联合物体识别网络（3D ORN）用于提取<u>三维的几何特征和二维的颜色特征</u>。然而，这两个模块的计算是单独进行的，不共享卷积层。此外，在大的三维搜索空间中的三维体素网格上操作3D ConvNets，通常计算成本很高。

**最近，从二维角度利用三维物体检测的方法得到了研究者的关注。**

✅2D-Drievn-3D-Frustum[21]提出了一种二维驱动的三维物体检测方法，以减少三维物体的搜索空间，该方法使用一个**多阶段的管道来进行二维物体检测、三维物体方向回归和基于上下文信息的物体重构。**然而，他们使用<u>手工制作的特征（即三维点的坐标直方图）来训练多层感知器（MLP）网络来回归三维边界盒。</u>

✅2D-Driven-3D-Regress[8]也提出了一种通过从二维推断出三维边界框的三维物体检测技术。他们的工作的局限性在于它不是完全集成的架构，因为有两个独立的计算：<u>首先，他们通过使用扩展的多尺度组合分组（MCG）算法[14]，从外部计算二维边界框建议及其分割掩码，</u>然后他们使用这些二维建议和相应的分割掩码信息来计算用于分类和<u>三维边界框回归的三维盒子。</u>然而，<u>计算二维建议和分割掩码的oﬄine算法（即MCG[14]）需要相当高的计算成本，对自动系统来说是不可行的。</u>**此外，他们需要在训练和测试阶段都提供物体分割信息，以预测三维边界框。**

在本文中，我们的目标是设计一个有效的三维物体检测框架：

- 该框架可以**消除高计算量的点云处理以及预测三维空间中物体的三维边界框所需的分割掩码信息。**
- 我们遵循从二维视角检测三维物体的概念，设计了一个整合的**端到端架构**，而不是分片计算。

特别是，我们的物体检测网络可以被看作是最近在RGB-D图像中最先进的3D物体检测网络的简化[44, 8]。

## 实验验证

在本节中，我们进行了大量的实验来验证我们提出的三维物体检测方法的性能。

### 4.1. 数据集

NYUv2 + Sun RGB-D

我们在两个标准数据集上评估了我们的三维物体检测框架：第一，NYUv2数据集[40]，由[8]提供改进的注释；第二，Sun RGB-D数据集[42]。

标准NYUv2数据集总共包括1449对由微软Kinect v1拍摄的对齐的RGB和深度图像的密集标记，其中795张图像用于训练，654张图像用于测试。另一方面，SUN RGB-D数据集包含10335张RGB-D场景图像，其中5285张用于训练，5050张用于测试。

如图2所述，我们采用「原始深度图像」来计算<u>三维边界框中心</u>，并将「零填充深度图像」作为CNN的输入来<u>训练网络</u>。

<img src="https://oj84-1259326782.cos.ap-chengdu.myqcloud.com/uPic/2021/05_31_image-20210531110443240.png" alt="image-20210531110443240" style="zoom:50%;" />

图二：三维包围盒的生成

1. **中心点**：给定一个来自MF-RPN的二维边界框（显示在红色矩形区域），我们首先将其按比例系数**缩小**（显示在深度图像上绿色虚线的矩形区域）。然后，三维边界盒的中心点（x0 , y0 , z0 ）被计算为从二维盒子的中心点**投影**出来的三维点，并缩小为区域的深度裁剪的中值。
2. **尺寸**：(l, w, h)代表长度、宽度和高度，初始化自训练集计算出的平均类盒**尺寸**，
3. **方向**：黄色虚线决定了三维盒子的**方向角**。

### 4.2 评价指标

IOU、Ap、mAP

我们采用了[43]中定义的传统三维体积交叉联合（IoU）指标。如果与地面实况的IoU大于0.25，则检测到的界线盒被认为是真阳性。我们分别按照[44]和[37]的方法对NYUv2数据集的19类物体检测任务，即浴缸、床、书架、盒子、椅子、柜台、桌子、门、梳妆台、垃圾桶、灯、显示器、床头柜、枕头、水槽、沙发、桌子、电视和厕所，以及SUN RGB-D数据集的10类物体检测任务，即浴缸、床、书架、椅子、桌子、梳妆台、床头柜、沙发、桌子和厕所。最后，在计算每个类别的平均精度（AP）后，我们计算平均平均精度（mAP）进行性能评估。

## 4.4. 与最先进的方法进行比较

在本小节中，我们进行了几个实验来评估所提出的3D物体检测框架在NYUv2[40]和SUN RGB-D[42]数据集上的性能，并将其结果与最先进的方法进行比较。

**NYUv2** - DSS & 2D-Driven-3D Regress

表1显示了我们的3D物体检测器在NYUv2测试集中的表现。在这里，我们与之前最先进的方法DSS[44]和Deng[8]在19类物体检测任务上进行比较。

结果清楚地表明，我们在速度和准确度方面都远远超过了最先进的方法。特别是，检测结果提升到mAP43.1，比DSS[44]提高了6.8%，比[8]提高了2.2%。此外，我们的方法明显提高了每幅图像的检测速度，比DSS[44]和[8]分别快65倍和2.5倍。我们的方法在NYUv2数据集的19类物体上取得了最佳效果。我们发现，实验结果明显优于DSS[44]和[8]，特别是在小物体上，类别：箱子、柜台、桌子、灯、显示器、床头柜、桌子和电视。在DSS[44]中，由于Kinect深度数据的噪声和不完全性，三维锚定框在可能是稀疏或空的点云上滑动，这导致三维锚定框错过了点云中的稀疏物体。

1. <u>Amodal detection of 3d objects: Inferring 3d bounding boxes from 2d ones in rgb-depth images [8]</u> 使用物体实例的**分割掩码信息来初始化三维框**，这导致在训练/测试时为每个物体实例提供分割像素信息。
2. 相比之下，我们只是使用从MF-RPN生成的**二维建议**，并以**二维边界框的减少区域的深度值初始化三维边界框**。MF-RPN和3D物体检测器都是从颜色和深度特征中学习的，它们都不依赖于点云或物体段信息。

从实验中，我们发现比例因子0.3的表现更好（在第4.5节中描述），因为缩小的区域大部分覆盖了物体的中心区域，可以正确地初始化三维框的中心点。

![image-20210531112406129](https://oj84-1259326782.cos.ap-chengdu.myqcloud.com/uPic/2021/05_31_image-20210531112406129.png)

**SUN RGB-D** - COG、LSS、DSS、2D-driven-3D Frustum

表2显示了我们在SUN RGB-D数据集上提出的方法的定量结果。继COG[37]之后，我们将10类物体检测任务与四种最先进的方法进行比较。COG [37], LSS [38], DSS [44], 和2D-driven [21]。我们直接从[37]、[38]、[44]和[21]报告结果和运行时间。

与NYUv2数据集一样，这里我们介绍了我们的技术使用比例因子0.3的结果。

从表中可以看出，我们的方法比2D驱动[21]快13.8倍，比DSS[44]快65倍，比COG[37]和LSS[38]快三个数量级，同时达到了相当的检测性能，比[21]和[44]分别提高了0.58%和3.6%。与COG[37]和LSS[38]相比，我们的方法没有在点云中详尽地搜索三维边界框，这需要额外的计算时间。我们直接从检测到的2D盒子中计算出3D边界盒。**我们的3D检测器的性能取决于2D检测。**有时，如果二维检测器由于暗光或遮挡而错过了物体，相应的三维物体也将无法检测到。此外，<u>正如在[8]中所报告的，直接将二维规则应用于由[42]提供的带有二维注释的SUN RGB-D，由于该数据集中的二维注释问题（更多描述见[8]），导致了一个严重的问题。然而，我们仍然设法在速度和准确性方面取得了相当的性能。</u>

![image-20210531151701383](https://oj84-1259326782.cos.ap-chengdu.myqcloud.com/uPic/2021/05_31_image-20210531151701383.png)

值得注意的是，我们的方法无需从点云处理开始，也无需使用任何需要额外高计算量的分割建议，就能实现这一结果。此外，我们的方法**缓解了在点云上的三维搜索空间的大量计算**问题，并且不需要对二维分割建议的生成进行任何额外的offline处理。

在图3中，我们直观地展示了NYUv2测试数据上一些有代表性的**真阳性**检测结果，这些数据的类分大于阈值（0.7）。

![image-20210531120146880](https://oj84-1259326782.cos.ap-chengdu.myqcloud.com/uPic/2021/05_31_05_31_image-20210531120146880.png)

我们还在图4中展示了NYUv2数据的一些**错误检测结果**，其中错误分为四种类型：错误的类别、方向错误、不准确的位置和错误的盒子大小。<u>（位置、大小、方向、类别）</u>

![image-20210531120311638](https://oj84-1259326782.cos.ap-chengdu.myqcloud.com/uPic/2021/05_31_image-20210531120311638.png)

## 结论 

由于缺乏大规模的注释训练数据，RGB-D图像中的三维物体检测是一项非常具有挑战性的任务。为了应对这一挑战，在这项工作中，我们提出了一种用于室内三维物体检测的快速而有效的算法。

我们主要做了两个贡献。首先，我们设计了一个多模态融合区域建议网络（MF-RPN）来预测二维物体建议；其次，我们提出了一种古老的技术，通过缩小二维边界框，从二维物体建议中生成三维建议。

与许多最先进的工作不同，我们的方法**不使用多阶段**点云处理，也**不使用任何预先计算的分割信息**。给定一对颜色和深度，我们的系统可以在其**全部范围内预测多个三维物体**。与最先进的3D物体检测器相比，我们的方法在具有挑战性的NYUv2数据集上实现了更快、更好的检测性能，而且在大规模的SUN RGB-D数据集上仍有相当的表现。今后，我们将在实时三维物体检测和跟踪等任务上扩展并验证所提方法的有效性。

