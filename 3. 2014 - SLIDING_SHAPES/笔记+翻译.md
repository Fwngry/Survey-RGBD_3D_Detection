# 2014 - Sliding Shapes

> 纯3D方法 （不是把2D方法推广到3D）
>
> 物体检测+深度图分割
>
> Sliding shapes用于解决挑选杂乱物体
>
> CG模型很好地解决了遮挡与信息缺失

Pipeline：CAD模型 - 多视角渲染深度图 - 生成点云并用于训练SVM分类器（每次渲染CAD - 合成深度图 - 提取一个点云特征）- （测试+负阴性挖掘）在三维空间中使用“3维检测窗口” - 图像分割提高性能

## 摘要

RGB-D传感器的深度信息大大简化了计算机视觉中的一些常见挑战，使一些任务取得了突破性进展。在本文中，我们建议使用深度图进行物体检测，并设计一个三维检测器来克服识别的<u>主要困难，即纹理、光照、形状、视点、杂波、遮挡、自我封闭和传感器噪声的变化。</u>

我们收集了一组三维CAD模型，并从数百个视角渲染每个CAD模型以获得合成深度图。对于每个深度渲染，我们从三维点云中提取特征并训练一个Exemplar-SVM分类器。在测试和硬阴性挖掘（hard-negative mining）过程中，我们在三维空间中滑动一个三维检测窗口。

实验结果表明，我们的3D检测器在RGB和RGBD图像上都明显优于最先进的算法，与DPM和R-CNN相比，平均精度提高了约×1.7。

## 1 Introduction

[模式匹配不够用]：由于许多原因，如遮挡、观察角度的变化和部件的衔接，用图像模式进行模板匹配对三维场景分析是不够的。图像模式不是不变的。

尽管在 *<u>图像补丁分类(image patch classiﬁcation)</u>* 方面取得了快速进展[2-6]，但物体检测仍然是一个开放的研究挑战。同时，廉价的RGB-D传感器的出现，如微软Kinect、苹果PrimeSense、英特尔RealSense和谷歌Project Tango，大大简化了视觉中的一些常见挑战，并使一些任务取得了突破，如<u>身体姿势估计[7, 8]、内在图像[9]、分割[10, 11]和三维建模[12]</u>。在本文中，我们提出了一种使用深度图像进行一般物体检测的算法，与RGB图像[2]上的最先进结果相比，我们取得了明显的性能改进。

其主要思想是以数据驱动的方式利用深度信息来克服物体检测中的主要困难，即纹理、光照、形状、视角、自我遮挡、杂波和遮挡的变化。对于一个特定的物体类别（如椅子），我们使用互联网上的计算机图形（CG）CAD模型。我们从数百个视点渲染每个CG模型，以获得合成深度图，就像它们被一个典型的RGB-D传感器观看一样。如图1所示，对于每次渲染，我们从与渲染的深度图相对应的三维点云中提取一个特征向量来训练一个示例支持向量机（SVM）[3]，使用来自RGB-D数据集[10]的负面数据。在测试和 <u>*硬阴性挖掘过程hard-negative mining*</u> 中，我们在三维空间中滑动一个三维检测窗口 slide a 3D detection window in the 3D space，以匹配 <u>*典型的形状和每个窗口(the exemplar shape and each window)*</u> 。最后，我们使用深度图分割来进一步提高性能。

我们设计的成功是基于几个关键的见解。

1. 为了处理纹理和光照的差异，我们使用深度图而不是RGB图像。
2. 为了处理形状差异，我们使用了一种数据驱动的方法来利用涵盖现实世界中形状差异空间的CG模型集合。我们还在CG模型的大小上增加了一个小的差异，以提高鲁棒性。
3. 此外，与 <u>?直接网格对齐[13]?</u> 相比，使用正反两方面的数据学习SVM也增加了检测器的通用性。
4. 为了处理视点差异，我们可以密集渲染物体的不同视点，以覆盖所有典型的观察角度。
5. 为了处理深度传感器的误差和噪声，我们使用CG模型来获得完美的渲染，并将其作为积极的训练数据（表2中的实验表明，它有很大的帮助）。为了弥补CG训练数据和RGB-D测试数据之间的领域差距，我们渲染深度图（但不是颜色），就像从典型的RGB-D传感器看CG模型一样。
6. 为了处理杂乱无章的物体（例如一把椅子的座位在桌子下面），我们使用带有掩码的三维滑动窗口来指示哪些部分应该在分类过程中被考虑。
7. 为了处理物体间的遮挡，我们利用深度图来推理遮挡的来源，并将遮挡的区域视为丢失的数据。为了利用自我遮挡，我们渲染CG模型并计算截断符号距离函数（TSDF）[14]作为一个特征。

训练CG+测试SS

由于我们的通用物体检测器不依赖于任何关于背景的假设，也不需要一个主要的支持平面[15, 16]，而且它的单视图性质不需要对物体进行（半）完整的扫描[17]，**所以它可以作为一般场景理解任务的基本构件。**为了提高三维卷积过程中的测试速度，我们将积分图像[18]推广到三维，以跳过空窗。

在下面的章节中，我们将更详细地描述我们的算法。

在第3节中，我们将谈论评估指标和实验来评估该算法。

在第4节，我们将讨论我们提出的方法与现有方法的关系。

我们使用CG模型的集合来训练一个3D检测器。对于每个CG模型，我们从数百个视图角度对其进行渲染，以产生一个积极的训练数据库。对于每张渲染图，我们训练一个Exemplar-SVM模型。我们将所有来自CG椅子模型渲染的SVM集合起来，建立一个3D椅子检测器。

## 2 The Sliding Shapes Detector

训练学习一个线性Exemplar-SVM分类器的集合（第2.1节）每个分类器都是用CG模型的渲染深度图作为单一的正面和许多负面的标记深度图来训练的。

在测试过程中（第2.2节），我们把带有重力方向的深度图像作为输入。学习到的SVM被用来对三维的滑动窗口进行分类，并输出带有检测分数的三维边界框。

我们设计了四种类型的三维特征（第2.3节），并提出了几种方法来处理杂波、遮挡和深度缺失（第2.4节）。

### 2.1 训练

渲染深度图 - 训练Exemplar-SVMs

图2显示了我们的滑动形状检测器的训练过程，我们将每个视角的渲染作为一个范例，并为其训练一个单独的分类器。

**1. 渲染深度图为CG模型**

对于每个物体类别，我们从互联网上收集一组具有典型形状的CG模型，以覆盖类别内的形状差异。因为现实环境中的大多数物体都有一些**支撑面**（例如，椅子通常在地板上），我们在渲染图形模型时也会合成一个支撑面来模拟这种情况。对于每个CG模型，我们从不同的视角和三维空间的位置进行渲染。

具体来说，我们在渲染CG模型时改变了以下参数：方向、比例、3D位置和相机倾斜角度。基于数据集的统计和观察，**我们做了一些假设**，以减少样本空间。我们假设大多数物体在重力方向上对齐，所以只有围绕重力轴的旋转。我们还从训练集中获得了**每个类别的物体大小和三维位置的统计数据**，并在此基础上对视点参数进行采样。

除了对上述参数进行采样外，我们还**对网格进行了轻微的缩放以提高鲁棒性**。

最后，我们通过使用相同的相机内在参数和分辨率来渲染深度图，就像从一个Exemplar - RGB-D传感器看CG模型一样。

**2. 训练Exemplar-SVMs**

如图1所示，在将CG模型的每个深度渲染转换为三维点云后，我们提取一个特征向量并将其作为正数来训练线性Exemplar-SVM[3]。最初的负数是从有注释的Kinect图像（RMRC数据集[10, 19]）中随机挑选的点云，这些点云不与地面真实的正数重叠。我们通过在整个训练集上搜索硬底片来进行硬底片挖掘。

**3. 无校准（略）**

尽管我们分别训练每个Exemplar-SVM，但我们没有像[3]那样校准我们的检测器，主要是因为RMRC数据集[10, 19]的规模有限。校准要求训练（或验证）集具有与测试集相似的正向分布：大部分的典范应该至少发生一次，以便相应地调整它们的分数。在我们的案例中，每个CG模型中的每个view都有一个Exemplar-SVM。

Exemplar模型的总数在很大程度上超过了RMRC数据集中阳性物体实例的总数，一些检测器在训练集中永远不会出现，这使得校准成为不可能。

### 2.2 测试

在测试过程中，我们使用所有的Exemplar-SVMs对三维空间中每个可能的边界盒进行详尽的分类，每一个都评估相应的形状是否存在于边界盒内，并输出一个检测分数。然后，我们对三维空间的所有检测框进行非最大抑制。

**三维局部搜索（3D Local Search）：**给定一个在相对于虚拟摄像机的特定三维位置渲染的CG模型上训练的示例-SVM，<u>我们只在附近进行三维卷积</u>。**（解释）**这种对搜索空间的限制提高了速度和检测的准确性，因为「远离训练地点的物体具有不同的点密度，并且由于其视角的不同而呈现出不同的自我封闭条件」。<u>SVM和三维特征可能不够强大，无法对这种差异进行建模</u>。因此，我们采取了一个更保守的搜索，只限制在附近的位置。

**跳跃窗口：**在三维中，有很多空的空间可以被安全地跳过。为了识别空箱并在卷积过程中跳过它们，为每张测试图像计算了一个三维积分图像，其中每个单元存储了该单元左上角的所有单元的点数之和。

在卷积过程中，给定一个模型的窗口大小和它当前的单元位置，这个窗口内的总点数可以在恒定时间内从三维积分图像中快速计算出来。如果这个窗口内的总点数小于50，我们的检测器就会跳过这个窗口而不进行点乘。

**界限盒调整：**最初形成的卷积边界框与定义的特征轴对齐，这对大多数物体来说不是最佳的。因此，在我们得到与轴对齐的边界框后，我们用与物体的原理轴对齐的更紧密的边界框来代替它，这些边界框是从CG模型中导入的。

### 2.3 三维特征

为了支持在三维空间中滑动窗口，三维空间被划分为大小为0.1米的立方体单元，并从每个单元中提取若干特征。为了捕捉三维物体的属性，如它们的几何形状、方向和与摄像机的距离，我们设计了以下特征，并将所有这些特征结合起来，形成一个判别性的描述符。

1. 点密度特征：为了描述cell内的点密度分布，我们将每个细胞划分为6×6×6个体素，并建立每个体素中的点数量的直方图。一个三维高斯核被用来对每个点进行加权，以消除体素离散化的偏差。在获得细胞内的直方图后，这是一个216维的向量，我们随机挑选1000对条目并计算每对条目的差异（受[7]中的棍子特征启发）。然后将棍子特征与原始计数直方图串联起来。这样的描述符同时捕获了点云的一阶（点计数）和二阶（计数差异）统计。

2. 三维形状特征：（体素内分布-主成分）除了点在体素间的分布，它们在体素内的分布也是重要的线索，我们用局部三维形状特征来编码。我们将每个单元划分为3×3×3个体素，并通过点云的主成分（假设点的协方差矩阵的特征值为λ1>λ2>λ3）来表示体素的内部点云分布。

3. 三维法线特征：表面法线对于描述一个物体的方向至关重要。为了计算三维法线，我们为每个点挑选25个最近的邻居，并估计该点的表面法线为第一主成分的方向。我们将半球的方向均匀地划分为24个仓，对于每个单元，我们建立一个跨这些仓的法线方向直方图作为法线特征。

4. TSDF特征：
   1. 自闭 Self-occlusion 是基于视图的形状匹配的一个有用线索。我们采用截断符号距离函数（TSDF）[14]作为特征之一。
   2. TSDF特征是对全局形状的体积测量。对于划分为6×6×6体素的每个单元，每个体素的TSDF值被定义为体素中心与摄像机视线上最近的物体点之间的符号距离。该距离被剪切为-1和1之间，这里的符号表示cell是否在表面的前面或后面。
   3. 在计算每个体素的TSDF值后，我们使用与点密度特征相同的random-stick来计算成对的差异，并将其与原始TSDF向量连接起来。
5. 特征编码和组合：我们在每个特征之上进行字典编码[20]。具体来说，我们使用k-means，为每种类型获得50个聚类中心，作为我们的编码本。然后，每个特征向量x被编码为一个50维的向量f，包含其与50个中心的距离。特征编码后，我们将所有编码的特征向量连接起来，以得到最终的组合特征向量。
6. 特征的可视化

## 3 Evaluation

我们用于训练的3D CG模型是从Trimble 3D仓库收集的。我们在RMRC数据集（NYU Depth v2[10]的一个子集）上评估我们的滑动形状检测器。我们选择了五种常见的室内物体：椅子、马桶、床、沙发和桌子，并手动查看注释，以确保它们被正确标记。我们将RMRC数据集分成500张深度图像用于训练，574张深度图像用于测试。我们分割数据集的方式是将同一视频中的图像归为一组，放在训练集或测试集两者之一，并试图平衡每个类别的训练集和测试集的实例数量。

我们的算法将「来自RGB-D传感器的深度图像与重力方向」作为输入。将点云和CG模型与重力方向对齐，就可以使用轴对齐的滑动窗口进行检测。重力方向可以通过几种方式获得：如果一个RGB-D相机安装在机器人上，我们知道机器人的配置和它的相机倾斜角度。对于移动设备上的相机，我们可以使用加速度计来获得重力方向和相机的相对倾斜角度。在本文中，RMRC数据集的重力方向被作为gt提供。对于没有地面实况重力方向的数据集，也很容易通过对地板和墙壁的平面化来计算[22]。

如果没有局部搜索和跳跃窗口，我们的时间复杂度与Exemplar-SVMs[3]完全相同。平均而言，每幅图像有25,058个三维检测窗口。本地搜索将其减少到19%。跳跃窗口将其减少到44%。使用这两者，它减少到8%。对于测试来说，在Matlab中对一个深度图像进行测试，每个检测器大约需要2秒。除了检测结束时的非最大抑制，计算自然是可并行的。对于训练，在Matlab中用单线程训练一个检测器需要4到8小时，这也是自然可并行的。

### 3.1 评价指标

指标：我们采用PASCAL VOC[23]中的标准二维物体检测评估方案，并作了如下修改。PASCAL VOC的评价标准采用二维边界盒重叠率（IOU），假设它们与图像轴对齐。对于三维，我们计算**三维包围盒的重叠率**，我们假设包围盒与重力方向对齐，但对其他两个轴没有假设。

比较：为了与二维检测进行比较，对二维的评估是通过将地面实况和检测框**投射到二维中，并计算它们的二维重叠率来完成的。**

阈值：对于二维来说，如果重叠率超过0.5，预测的盒子被认为是正确的。为了让相同的结果在二维和三维评估中产生相似的精度-召回曲线，我们将**三维的阈值设置为0.25**。

困难：与PASCAL VOC类似，我们也添加了一个困难标志，以表明gt是否难以检测。困难的情况包括严重遮挡、深度缺失和视线之外。我们分别对正常的gt box（表示为3D和2D）以及包括困难情况在内的所有gt box（表示为3D+和2D+）进行评估。

### 3.2 实验

图5显示了我们的滑动形状检测器的例子结果，图10和11显示了一些失败案例。我们的检测器不仅能识别物体，还能识别它的方向和三维风格的类型，这是从提出检测的相应模型中导入的。

图8展示了我们设计的作用。

第1行和第2行是我们的检测器成功处理遮挡的案例。

第3行显示，占位掩码可以过滤掉杂波，其中一张餐桌部分位于提议的盒子内，但它并不影响椅子检测器，因为它不在占位掩码中。

第4行显示了严重信息缺失。即使椅子的整个背部都丢失了，我们的检测器也能够检测到它，尽管相应的CG模型与物体不完全相同。

对比：我们将我们的滑动形状检测器与2D和3D检测器进行定量比较。在二维情况下，我们与标准的DPM[2]和最先进的深度学习算法RCN[6]进行比较。我们展示了DPM在PASCAL VOC 2010[23]、SUN2012[26]和RMRC数据集上的训练结果。

表2显示了平均精度。与所有RGB算法中最好的算法相比，我们的方法在平均精度上取得了大约×1.7的改进，并且也优于其他RGB-D或3D检测器。

**选择CG模型作为训练数据的合理性：Kinect与CG模型** - Table2

为了证明我们选择CG模型作为训练数据的合理性，我们使用Kinect点云作为正面训练数据来评估性能。这些点云是从训练集的真实情况中挑选出来的，这些点云被标记为非困难的（以避免严重的遮挡或巨大的深度丢失）。然后，我们使用完全相同的特征、负面数据和训练程序，为每个正面的Kinect点云训练一个Exemplar-SVM。在我们提出的方法中，大量的渲染的CG模型被用作训练数据。为了实现公平的比较，我们限制了CG训练集的大小，使其不大于Kinect点云：对于每个Kinect点云的正面例子，我们挑选与之最相似的渲染的CG模型并添加到CG训练集。这是通过在训练集上测试所有的CG模型来完成的，并挑选出对每个正面地面真相具有最高可信度的模型。因此，<u>挑选的CG模型的总数只能小于Kinect阳性的总数</u>，因为多个Kinect阳性可能对应于一个CG模型。

在表2中[Kinect对齐]，我们显示，即使有较少的阳性训练数据，在CG模型上训练的检测器仍然表现得很好。**我们认为，真实的Kinect深度数据由于其在传感器噪声、缺失深度、遮挡和背景杂波方面的高变化，作为正面例子是比较差的。**

（Kinect解决遮挡问题，必须要有雷同的遮挡样本用于学习；CG有完整的数据可用于对照）例如，为了使点云与某些部分被遮挡的样本很好地匹配，点云必须具有类似的遮挡条件，否则可用于匹配的部分将是不充分的，而如果正面数据是完整的，只要有足够部分的可见部分，候选点云可以更加灵活。

此外，如果两个物体实例不是在完全相同的条件下拍摄的，那么它们具有相似的传感器噪声是非常罕见的。通常情况下，从背景中分离出来的物体或另一个数据集上训练的分类器性能较差。然而，**我们的算法能够弥补CG训练数据和RGB-D测试数据之间的领域差距，并取得明显的改善。**

**我们还测试了使用CG模型和Kinetic点云作为正数来训练检测器的组合。**表2[Kinect+CG]显示，它并没有产生比单独使用CG模型更好的性能，这表明信息是多余的，Kinect模型的点云质量不好。

**例子的数量**-FIG 9

我们实验了正向训练数据的大小（GC模型的数量和视点渲染的数量）如何影响性能。给定训练视点/模型的数量，我们随机挑选5个可能的案例来评估平均精度。图9显示了当渲染视点的数量和CG模型的数量发生变化时，平均精度如何变化。

## 4 Related Works and Discussions

我们的工作受到了图像、范围扫描、深度图、RGB-D和CAD模型的物体识别研究的启发，但我们在这里只提及最相关的研究。

基于图像的检测：流行的检测器通常在**窗口内的图像区域训练分类器，并通过滑动窗口[2, 3, 27]或选定区域[4-6]使用分类器进行测试。**<u>解释物体变化</u>的典型方法是具有图像结构的可变形部分[2]和典范Exemplar的集合[3]。[28-30]表明，后一种方案要简单得多，并具有对所有物体类别的可推广性。

我们的模型可以被理解为将这一框架扩展到三维的一种新方法。也有一些关于使用CAD模型进行训练的工作[31-36]，但它们不是针对深度图像。

**语义分割：(分割：自下而上；SS：全局)**

一种流行的制定三维物体识别的方法是预测深度图或三维网格的每个区域的语义标签[37, 38, 11, 10, 3944]。由于**自下而上**的性质，这些算法只能看到物体的一部分，而不是整个物体。基于滑动窗口的方法的一个优点是使分类器能够 **使用整个物体的信息** 来做决定。

**投票：（整合局部信息+难以加权）**

有许多工作集中在如何通过投票来 **整合局部信息** [20, 45-49]，如Hough投票或隐式形状模型。这类模型可以同时考虑多个局部区域进行物体识别，但<u>要在数据驱动的机器学习框架中制定它们，对物体各部分之间的相对重要性和相关性（尤其是负相关）进行加权是很困难的。</u>

**关键点匹配：（三维关键点匹配+描述符：非学习性质）**

正如用于图像匹配的SIFT关键点[50]，一种流行的算法类型[51-60]是检测三维点云或网格上的关键点，为关键点生成描述符（如旋转图像和三维形状背景），并使用匹配来与训练数据中的模型对齐。与基于投票的方法一样，这种算法的**非学习性质**使得它很难从数据中辨别学习不同关键点的重要性。

**模型拟合：（不使用描述符+非数据驱动）**

与关键点匹配相似，模型拟合算法将输入与训练模型对齐[61]，但不使用描述符。有一些稳健的算法可以将三维形状与场景相匹配[62-64]。但是，由于非数据驱动的性质，这些方法也有同样的问题，即<u>它不能从数据中学习</u>。

三维分类法：基于分类的方法[65-68, 51, 69-79]通常同时考虑整个物体，通过提取整个物体的整体特征并通过分类器对特征向量进行分类。但典型的设置是将分割后的物体作为输入（甚至是具有完整网格的单独三维模型），并将物体分类到其中一个固定的类别中，这比需要定位物体和区分非物体窗口的物体检测要容易得多。

**2.5D检测器：（2D detector 拓展到可使用深度图depth map）**

有几项开创性的工作试图将标准的基于2D图像的物体检测器扩展到使用深度图[24, 80, 81, 25, 82]。<u>主要的区别是我们的算法完全在三维中运行，使用三维滑动窗口和三维特征，可以自然地处理遮挡和其他问题。</u>

**RGB-D场景理解：（应用场景）**

除了上面提到的RGB-D分割和检测工作，[83, 19, 84]提出了估计房间布局、支持面和对整个房间包括物体的场景理解。我们的三维检测器可以作为所有这些更高层次任务的物体检测的基本构建模块。