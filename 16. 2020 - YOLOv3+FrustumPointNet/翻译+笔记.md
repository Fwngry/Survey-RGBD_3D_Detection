> 使用YOLO v3，快速
>
> 相比于f-pointnet，二维信息作为额外的通道被充分使用，CNN可以学习额外的特征；模态三维箱体估计网络也再次使用二维信息。

## 摘要

本文旨在从点云中对室内和室外场景进行实时的高精度三维物体检测，只使用一个RGB-D相机。我们提出了一个新的网络系统，该系统结合了二维和三维物体检测算法，以达到更好的实时物体检测效果，并通过简化我们在真实机器人上的网络而拥有更快的速度。YOLOv3是最先进的基于2D图像的物体检测方法之一。

Frustum PointNets是一种使用frustum约束来预测物体的三维边界盒的实时方法。结合这两种方法可以有效地进行实时的2D-3D物体检测，无论是室内还是室外。

我们不仅提高了训练和评估精度，降低了KITTI物体检测基准的平均损失，而且在三个不同难度级别的所有类别的三维检测中取得了更好的平均精度（AP）。

此外，我们还在三种不同的硬件设备上实现了我们的机载实时2D和3D物体检测系统，只使用了一个RGB-D相机。

![image-20210627125705031](https://oj84-1259326782.cos.ap-chengdu.myqcloud.com/uPic/2021/06_27_06_27_image-20210627125705031.png)

## 一、引言

现在，有很多优秀的基于深度卷积神经网络（CNN）的二维和三维物体检测方法，它们可以通过图形处理单元（GPU）加速来实现实时性，例如YOLO（Redmon和Farhadi，2018；Redmon等人。2016；Redmon等人，2016）、Faster R-CNN（Ren等人，2015）、Mask R-CNN（He等人，2017）和PointNets（Qi等人，2018；Qi等人，2017a；Qi等人，2017b）。尽管如此，对3D物体的准确和快速检测是一个挑战，在机器人技术中起着重要的作用，特别是对于自主驾驶。自主机器人被广泛应用于不同的研究领域，可以配备不同的传感器和强大的GPU。**RGB-D相机被广泛使用，与雷达或激光雷达传感器相比，它们更便宜、更轻便，同时提供丰富的场景信息，如3D彩色点云。具体来说，RGB图像为我们提供了有用的二维信息，可用于最先进的二维物体检测方法，而深度图对三维信息是有用的，如点云的几何位置，它提供了更多的物体几何信息。**

一些研究人员致力于研究结合二维和三维物体检测的方法，例如Frustum PointNets（Qi等人，2018）、MV3D（Chen等人，2017）、PointFusion（Xu等人，2018）和DenseFusion（Wang等人，2019）。与Frustum PointNets类似，我们的方法可以轻松有效地检测室外场景中的汽车和行人，这对自动驾驶和街道场景的理解非常有用。精确有效地检测室外场景中的汽车、行人和骑车人对自主驾驶和机器人避障很有帮助，而室内和室外场景中的人员检测对紧急救援和人机交互也很有帮助，并具有挑战性。

我们开发并分析了一个修改过的Frustum PointNet网络，它主要结合了二维和三维物体检测，以实现更好的点状物体检测精度，并在实时机器人应用上有更好的表现。我们还利用现有的KITTI物体检测基准（Geiger等人，2012）等基准，将我们提出的网络系统与Frustum PointNets算法进行比较。考虑到我们的系统具有良好的实时性能并可用于机器人，我们还在Robotnik Summit XL移动机器人以及Jetson TX2开发套件上实现了该系统。所有的测试实验都是在三种不同的硬件设备上进行和比较的。此外，在实际的室内和室外场景中，物体检测的性能也得到了提高。

我们在本文中的主要贡献是，我们提出了一个结合二维和三维物体检测方法的新策略，以达到更高的精度。另一个贡献是，我们在移动机器人中实现了二维和三维结合的物体检测，推理时间很快，而不是只在KITTI基准上进行训练和测试。最后但并非最不重要的是，我们的策略对室内和室外场景都很有效。

## 二、相关工作

### 2D检测

目前有很多流行的基于二维RGB图像的深度CNNs物体检测方法。二维物体检测主要可以分为两部分：基于区域的CNN，如Mask R-CNN（He等人，2017）和Faster R-CNN（Ren等人，2015）；一次性CNN，如YOLOv3（Redmon和Farhadi，2018）和SSD（刘等人，2016）。

faster R-CNN（Ren等人，2015）是一种采用区域提议网络（RPN）的端到端物体检测方法，可实时工作。它基于之前基于区域的fast R-CNN（Girshick，2015）的工作，作者用RPNs取代快速R-CNN的选择性搜索，后来通过共享卷积特征将RPNs和快速R-CNN合并为一个网络。Mask R-CNN（He等人，2017）是在Faster R-CNN的基础上，通过平行添加掩码网络扩展到图像分割中，因此它可以同时进行物体检测和分割。然而，即使有Nvidia GTX 1080Ti这样强大的GPU加速，它也只能达到每秒5帧（fps）。

**相反，YOLOv3（Redmon和Farhadi，2018）要快得多**，它是基于YOLOv1（Redmon等人，2016）和YOLOv2（Redmon和Farhadi，2017）的前期工作。YOLOv3是一种实时物体检测方法，它包含Darknet-53来提取特征，以及特征金字塔网络（FPN）来检测小物体。因此，它在检测不同尺度的物体方面取得了良好的性能。在同一类型的单一GPU加速下，它可以达到25 fps左右。**它的物体检测的二维边界框不仅可以通过去除特征匹配的一些离群值来提高跟踪精度（Wang和Zell，2018），还可以给出有用的语义信息，可能用于三维物体检测（Qi等人，2018）**。

### 3D物体检测

RGB-D相机可以为我们提供关于场景中物体的丰富信息。它们不仅提供二维RGB图像，还提供深度图，并且可以直接使用立体红外传感器获得三维点云。在（Wang and Zell, 2019a）和（Wang and Zell, 2019b）中，**作者使用安装在Metralabs Scitos G5移动机器人上的微软Kinect v2前视摄像头构建了RGB-D地图的彩色点云，他们表明彩色点云与仅有几何点云相比是有用的。**

大多数实时的最先进的3D物体检测方法是使用昂贵的激光雷达传感器，如Vote3Deep（Engelcke等人，2017），PointRCNN（Shi等人，2019）和Fast PointRCNN（Chen等人，2019）。一些基于图像的最先进的物体检测方法缺乏关于三维几何的信息（Su等人，2015；Kanezaki等人，2018；Johns等人，2016）。他们需要多视角图像或投影方法，如三角测量，这将导致测量的额外漂移误差。

### 二维和三维物体检测的结合

目前有一些最先进的方法将二维和三维物体检测结合起来。MV3D（Chen等人，2017）是一个基于多视角的3D物体检测网络，主要用于自主驾驶。该应用与我们的很接近，但我们只使用了一个RGBD相机，没有使用激光雷达传感器。作者结合了鸟瞰点云、前视点云和RGB图像信息，并且没有提到速度，所以我们猜测对于实时自主驾驶任务可能有一些空间。

Frustum PointNets（Qi等人，2018）是使用二维检测器得到一个二维盒子，然后投射到相应的地壳约束中，做三维盒子回归。PointNet和PointNet++（Qi等人，2017a；Qi等人，2017b）用于每个地壳内的实例分割，也用于正交的三维盒子估计。PointFusion（Xu等人，2018）提出了一种深度传感器融合方法，用于3D边界盒的估计。它使用RGB图像作为ResNet的输入来获得二维特征，并使用来自激光雷达传感器的相同的三维点云输入，然后将其放入PointNet，然后获得三维特征。密集融合和全局融合都是通过与最终模型和基线模型的比较来完成的。但上述方法都需要使用来自激光雷达传感器的三维点云，即在强度范围内具有良好的性能，但在没有强度的情况下性能不佳。此外，二维检测器部分没有开源代码，他们在KITTI基准中使用地面真实的二维检测结果进行训练。

**DenseFusion（Wang等人，2019）只使用RGB-D相机，它与PointFusion的密集融合部分类似，通过迭代估计6DoF物体姿势。它是一种像素级的密集融合方法，将颜色嵌入和PointNet的几何嵌入连接起来进行匹配点。与PointNets不同的是，他们使用平均池化而不是最大池化来获得全局特征。后来他们用同样的方法将局部点的特征与全局特征结合起来。作者还在一个可以抓取和操纵物体的机器人上演示了他们的方法，他们宣布他们的方法比PoseCNN+ICP快了大约200倍。然而，迭代最接近点（ICP）非常慢，很少有研究人员会将其用于实时应用。作者还宣布，他们可以在每一帧有5个物体的情况下，对丰田人力支援机器人达到16帧，但没有提到硬件GPU、CPU和内存信息。因此，不可能将我们的方法与他们的速度进行公平的比较。此外，对象的类别不同，应用也不同。**

## 三、系统设计

### 网络结构

简而言之，结合二维和三维物体检测的方法通常可以为我们带来更精确的检测结果，但可能会降低系统速度。因此，快速、准确的实时二维和三维物体检测是一个很大的挑战。我们希望找到一种新的、聪明的策略来结合二维和三维物体检测，最终实现对室内和室外场景的准确和快速检测。

### 数据预处理

对于基准，我们选择KITTI物体检测基准进行训练和评估。对泡菜文件的预处理与Frustum PointNets作者所做的相同。在KITTI（Geiger等人，2012）中，RGB图像是由PointGrey Flea2 RGB相机拍摄的，3D信息来自Velodyne HDL-64E 3D激光雷达传感器。校准文件也需要进行坐标转换和点投影。

对于在我们的机器人中的真正实施，我们主要使用英特尔Realsense D435 RGB-D相机。这种RGB-D相机重量轻，价格便宜，易于使用，并且有专门的文件，对于我们在室内和室外的真实环境中进行数据采集是非常方便和有效的。

由于我们选择了英特尔Realsense D435相机，所以使用了pyrealsense2。由于大量的点和一些噪声点，没有必要使用RGB-D相机的所有点云，所以我们对每个地壳进行逐帧随机下采样为2048个点。其原则是，我们不能过多地减少点，并保留原始点云的主要特征。

数据预处理主要有三个步骤。

二维边界框的提取、地壳提取和数据增强。二维边界盒可以直接从KITTI基准中提取，也可以由YOLOv3二维检测器从D435相机中接收。

然后，通过使用region2frustum函数，我们可以得到一个提取的地壳。

在数据增强中，二维盒子的中心被随机移位，宽度和高度被随机重新缩放，从0.9到1.1，用于我们的实际实现。物体的类型被表示为单热向量。最后，整个数据被保存为pickle文件，pickle文件包含ID、2D盒子参数（位置和尺寸）、3D盒子参数（位置和尺寸）、点云位置和强度、标签、物体类型、3D航向仓（偏航角）和地壳角等信息。

## 2D&3D检测

我们在本文中的主要贡献之一是，我们提出了一种将二维和三维信息纳入神经网络的新策略。Frustum PointNets首先使用二维物体的方法来获得二维边界框，然后使用这些信息来获得三维边界框并输入PointNets，进行实例分割，最后预测每个物体的三维边界框。

![image-20210627134011310](https://oj84-1259326782.cos.ap-chengdu.myqcloud.com/uPic/2021/06_27_06_27_image-20210627134011310.png)

> 我们只用一个RGB-D相机来获得3D点云，黄色部分突出了我们的主要差异。

经过多次试验，我们最终设计了如图1所示的系统。对于输入，点云的x、y、z位置以及来自二维物体检测的盒子的宽度和高度信息提供了有用的物体的先验信息。问题是，**我们需要获得二维箱体信息，并找到一个智能策略来融合二维和三维检测。**

最初的Frustum PointNets作者使用KITTI的地面实况二维盒子来训练他们的模型，而我们使用YOLOv3快速物体检测方法来获得二维盒子。尽管YOLOv3速度很快，但它没有Frustum PointNets作者设计的2D编码器-解码器那么精确，而且他们没有开源这部分代码，但通过他们的论文描述，他们使用了Fast R-CNN（Girshick，2015）和SSD（Liu等人，2016）的还原VGG（Simonyan和Zisserman，2014）基础网络，还添加了特征金字塔层（Lin等人，2017）。

![image-20210627134124530](https://oj84-1259326782.cos.ap-chengdu.myqcloud.com/uPic/2021/06_27_06_27_06_27_06_27_image-20210627134124530.png)

> 我们不仅添加了region2frustum的输出，和一个作为物体类向量的热向量k，而且还添加了二维边界框信息。

图2显示了我们的3D实例分割的子系统，黄色部分突出了我们与Frustum PointNets（Qi等人，2018）的主要区别。这里MLPs表示多层感知器，圆圈内的数字表示信念层的数量。每层都有ReLU的批量归一化。

对于三维实例分割，我们可以看到1024个特征向量并不是必须的。对于三维实例分割，与三维方框预测相比，**精度很高**（90%以上），而且我们注意到，即使将最终的全局特征向量从**1024个元素减少到512个元素也是足够的**。因此，我们改变了全局特征向量，从而节省了我们的训练和推理时间。此外，一些卷积层可以通过我们的多次测试**安全地删除**，整个系统的稳健性不会受到影响，这也**加快了我们整个系统的速度**。

![image-20210627135057782](https://oj84-1259326782.cos.ap-chengdu.myqcloud.com/uPic/2021/06_27_06_27_image-20210627135057782.png)

> 我们在这个子系统中不仅添加了3D实例分割的转换输出，而且还添加了组合的2D边界框信息。黄色部分突出了我们的主要区别。

图3显示了我们的三维实例分割的子系统。黄色部分突出了我们与Frustum PointNets（Qi等人，2018）的主要区别。

这里NS表示大小聚类的数量（默认值为8），NH表示航向分仓的数量（默认值为12）。FC表示全连接层，圆圈中的数字表示层的数量。

我们的系统比原来的Frustum PointNets系统更稳健、更准确，但如果我们只在系统内部增加更多的信息，那么速度会很慢。我们有一个实时性的要求，因为一个快速的实时系统对机器人的应用很有用。因此，我们需要做一些速度上的改进。总的来说，我们目前的系统在准确性和速度之间做了很好的权衡，

## 四、实验

![image-20210627145247347](https://oj84-1259326782.cos.ap-chengdu.myqcloud.com/uPic/2021/06_27_06_27_image-20210627145247347.png)

在本文中，我们设计了一个新的网络系统，它结合了二维和三维的有用特征，用于挑战性的物体检测任务。原来的Frustum PointNet只在KITTI基准上进行了训练和测试，并直接使用了2D物体检测结果。3D PointNet子系统并没有在其网络中重新使用2D信息。我们的简化网络系统（OurFPN）在三种不同的设备上，以及在KITTI基准上，比目前大多数最先进的3D物体检测网络有更高的准确性和更快的速度。

在不久的将来，我们将优化OurFPN以实现更高的精度和更快的速度。我们还打算使用激光雷达传感器作为补充，因为RGB-D相机的有效范围是有限的，这对于自主驾驶汽车的室外街道场景理解是不够的。事实上，我们注意到Velodyne可以得到相当远的点，如30米，但相当稀疏，对于近距离的点，如3米，效果不好。相反，RGB-D相机对于近距离的物体可以很好地工作，而对于远距离的点却无法检测。我们相信，将这两种类型的传感器信息结合在一起将是一种有效的方法，可以获得更好的物体检测结果。此外，我们将利用物体检测的三维边界盒信息来帮助改善无人机的SLAM系统，而现有的基于物体的SLAM系统只使用二维图像信息。